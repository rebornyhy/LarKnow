{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 1.171875,
      "learning_rate": 0.0004986111111111111,
      "loss": 1.3091,
      "step": 10
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 1.796875,
      "learning_rate": 0.0004972222222222222,
      "loss": 0.1134,
      "step": 20
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0004958333333333334,
      "loss": 0.057,
      "step": 30
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0004944444444444445,
      "loss": 0.0733,
      "step": 40
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 0.88671875,
      "learning_rate": 0.0004930555555555556,
      "loss": 0.0548,
      "step": 50
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0004916666666666666,
      "loss": 0.0549,
      "step": 60
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 0.2099609375,
      "learning_rate": 0.0004902777777777777,
      "loss": 0.037,
      "step": 70
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0004888888888888889,
      "loss": 0.0416,
      "step": 80
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.140625,
      "learning_rate": 0.0004875,
      "loss": 0.045,
      "step": 90
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 0.828125,
      "learning_rate": 0.0004861111111111111,
      "loss": 0.0229,
      "step": 100
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 1.453125,
      "learning_rate": 0.0004847222222222222,
      "loss": 0.0259,
      "step": 110
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.109375,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.0555,
      "step": 120
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 0.546875,
      "learning_rate": 0.00048194444444444446,
      "loss": 0.0554,
      "step": 130
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 1.34375,
      "learning_rate": 0.0004805555555555556,
      "loss": 0.0283,
      "step": 140
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.65625,
      "learning_rate": 0.0004791666666666667,
      "loss": 0.0443,
      "step": 150
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.1669921875,
      "learning_rate": 0.0004777777777777778,
      "loss": 0.0326,
      "step": 160
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 0.447265625,
      "learning_rate": 0.0004763888888888889,
      "loss": 0.0575,
      "step": 170
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.1416015625,
      "learning_rate": 0.000475,
      "loss": 0.0497,
      "step": 180
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 0.470703125,
      "learning_rate": 0.0004736111111111111,
      "loss": 0.0664,
      "step": 190
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.984375,
      "learning_rate": 0.00047222222222222224,
      "loss": 0.0371,
      "step": 200
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.2734375,
      "learning_rate": 0.00047083333333333336,
      "loss": 0.0427,
      "step": 210
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0004694444444444445,
      "loss": 0.0485,
      "step": 220
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 0.486328125,
      "learning_rate": 0.00046805555555555554,
      "loss": 0.0221,
      "step": 230
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.62109375,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.0347,
      "step": 240
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.8046875,
      "learning_rate": 0.0004652777777777778,
      "loss": 0.0314,
      "step": 250
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 0.59375,
      "learning_rate": 0.0004638888888888889,
      "loss": 0.0309,
      "step": 260
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.439453125,
      "learning_rate": 0.0004625,
      "loss": 0.0449,
      "step": 270
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.2333984375,
      "learning_rate": 0.00046111111111111114,
      "loss": 0.0262,
      "step": 280
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 0.040771484375,
      "learning_rate": 0.0004597222222222222,
      "loss": 0.017,
      "step": 290
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.5390625,
      "learning_rate": 0.0004583333333333333,
      "loss": 0.0406,
      "step": 300
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 0.40234375,
      "learning_rate": 0.00045694444444444444,
      "loss": 0.0389,
      "step": 310
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.0230712890625,
      "learning_rate": 0.00045555555555555556,
      "loss": 0.0254,
      "step": 320
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.671875,
      "learning_rate": 0.0004541666666666667,
      "loss": 0.0381,
      "step": 330
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0004527777777777778,
      "loss": 0.0302,
      "step": 340
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 0.5859375,
      "learning_rate": 0.0004513888888888889,
      "loss": 0.0204,
      "step": 350
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.640625,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.0305,
      "step": 360
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 0.1474609375,
      "learning_rate": 0.00044861111111111116,
      "loss": 0.0143,
      "step": 370
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 0.298828125,
      "learning_rate": 0.0004472222222222222,
      "loss": 0.027,
      "step": 380
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.875,
      "learning_rate": 0.00044583333333333335,
      "loss": 0.0275,
      "step": 390
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.359375,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.0712,
      "step": 400
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00044305555555555553,
      "loss": 0.0678,
      "step": 410
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.609375,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.0256,
      "step": 420
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 1.03125,
      "learning_rate": 0.00044027777777777777,
      "loss": 0.033,
      "step": 430
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.5078125,
      "learning_rate": 0.0004388888888888889,
      "loss": 0.0235,
      "step": 440
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.734375,
      "learning_rate": 0.0004375,
      "loss": 0.0245,
      "step": 450
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 0.326171875,
      "learning_rate": 0.00043611111111111113,
      "loss": 0.0266,
      "step": 460
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 0.2890625,
      "learning_rate": 0.00043472222222222225,
      "loss": 0.0156,
      "step": 470
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.25390625,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.0336,
      "step": 480
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 0.384765625,
      "learning_rate": 0.0004319444444444445,
      "loss": 0.0414,
      "step": 490
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.298828125,
      "learning_rate": 0.0004305555555555556,
      "loss": 0.0173,
      "step": 500
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.83984375,
      "learning_rate": 0.00042916666666666667,
      "loss": 0.0205,
      "step": 510
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 0.51171875,
      "learning_rate": 0.0004277777777777778,
      "loss": 0.0214,
      "step": 520
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 1.03125,
      "learning_rate": 0.00042638888888888886,
      "loss": 0.0208,
      "step": 530
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.212890625,
      "learning_rate": 0.000425,
      "loss": 0.0196,
      "step": 540
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.1083984375,
      "learning_rate": 0.0004236111111111111,
      "loss": 0.0255,
      "step": 550
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 2.15625,
      "learning_rate": 0.0004222222222222222,
      "loss": 0.0294,
      "step": 560
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.85546875,
      "learning_rate": 0.00042083333333333333,
      "loss": 0.0126,
      "step": 570
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 0.0576171875,
      "learning_rate": 0.00041944444444444445,
      "loss": 0.011,
      "step": 580
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 2.140625,
      "learning_rate": 0.0004180555555555556,
      "loss": 0.0226,
      "step": 590
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.0273,
      "step": 600
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 0.484375,
      "learning_rate": 0.0004152777777777778,
      "loss": 0.0228,
      "step": 610
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 0.25390625,
      "learning_rate": 0.0004138888888888889,
      "loss": 0.0161,
      "step": 620
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0004125,
      "loss": 0.0422,
      "step": 630
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.353515625,
      "learning_rate": 0.0004111111111111111,
      "loss": 0.0222,
      "step": 640
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 0.400390625,
      "learning_rate": 0.00040972222222222224,
      "loss": 0.0185,
      "step": 650
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.9453125,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.0126,
      "step": 660
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 0.93359375,
      "learning_rate": 0.0004069444444444445,
      "loss": 0.0103,
      "step": 670
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.61328125,
      "learning_rate": 0.00040555555555555554,
      "loss": 0.0383,
      "step": 680
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.61328125,
      "learning_rate": 0.00040416666666666666,
      "loss": 0.0408,
      "step": 690
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0004027777777777778,
      "loss": 0.0218,
      "step": 700
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 0.5859375,
      "learning_rate": 0.0004013888888888889,
      "loss": 0.0293,
      "step": 710
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.046142578125,
      "learning_rate": 0.0004,
      "loss": 0.0249,
      "step": 720
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 0.80859375,
      "learning_rate": 0.00039861111111111114,
      "loss": 0.0428,
      "step": 730
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0003972222222222222,
      "loss": 0.0281,
      "step": 740
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.435546875,
      "learning_rate": 0.0003958333333333333,
      "loss": 0.018,
      "step": 750
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 0.58984375,
      "learning_rate": 0.00039444444444444444,
      "loss": 0.0154,
      "step": 760
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00039305555555555556,
      "loss": 0.0232,
      "step": 770
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.52734375,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.018,
      "step": 780
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 0.5,
      "learning_rate": 0.0003902777777777778,
      "loss": 0.0344,
      "step": 790
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.10107421875,
      "learning_rate": 0.0003888888888888889,
      "loss": 0.0353,
      "step": 800
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.0201416015625,
      "learning_rate": 0.00038750000000000004,
      "loss": 0.0238,
      "step": 810
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 0.09130859375,
      "learning_rate": 0.00038611111111111116,
      "loss": 0.0155,
      "step": 820
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0003847222222222222,
      "loss": 0.0165,
      "step": 830
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.337890625,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.0274,
      "step": 840
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.64453125,
      "learning_rate": 0.0003819444444444444,
      "loss": 0.0175,
      "step": 850
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 0.76171875,
      "learning_rate": 0.00038055555555555553,
      "loss": 0.0425,
      "step": 860
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.05078125,
      "learning_rate": 0.00037916666666666665,
      "loss": 0.0097,
      "step": 870
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.3984375,
      "learning_rate": 0.00037777777777777777,
      "loss": 0.0064,
      "step": 880
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 0.111328125,
      "learning_rate": 0.0003763888888888889,
      "loss": 0.0238,
      "step": 890
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.042236328125,
      "learning_rate": 0.000375,
      "loss": 0.0105,
      "step": 900
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 1.34375,
      "learning_rate": 0.00037361111111111113,
      "loss": 0.0275,
      "step": 910
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.453125,
      "learning_rate": 0.00037222222222222225,
      "loss": 0.0262,
      "step": 920
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.11376953125,
      "learning_rate": 0.00037083333333333337,
      "loss": 0.0177,
      "step": 930
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0003694444444444445,
      "loss": 0.0238,
      "step": 940
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 0.76171875,
      "learning_rate": 0.0003680555555555556,
      "loss": 0.0137,
      "step": 950
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.625,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.0214,
      "step": 960
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 0.546875,
      "learning_rate": 0.0003652777777777778,
      "loss": 0.016,
      "step": 970
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.625,
      "learning_rate": 0.00036388888888888886,
      "loss": 0.02,
      "step": 980
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0003625,
      "loss": 0.0355,
      "step": 990
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.2578125,
      "learning_rate": 0.0003611111111111111,
      "loss": 0.0098,
      "step": 1000
    },
    {
      "epoch": 0.5611111111111111,
      "grad_norm": 0.1298828125,
      "learning_rate": 0.0003597222222222222,
      "loss": 0.0174,
      "step": 1010
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.0810546875,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.0192,
      "step": 1020
    },
    {
      "epoch": 0.5722222222222222,
      "grad_norm": 0.279296875,
      "learning_rate": 0.00035694444444444445,
      "loss": 0.0218,
      "step": 1030
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.216796875,
      "learning_rate": 0.00035555555555555557,
      "loss": 0.0079,
      "step": 1040
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.0849609375,
      "learning_rate": 0.0003541666666666667,
      "loss": 0.0105,
      "step": 1050
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 0.90625,
      "learning_rate": 0.0003527777777777778,
      "loss": 0.047,
      "step": 1060
    },
    {
      "epoch": 0.5944444444444444,
      "grad_norm": 0.2255859375,
      "learning_rate": 0.0003513888888888889,
      "loss": 0.0162,
      "step": 1070
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.466796875,
      "learning_rate": 0.00035,
      "loss": 0.0139,
      "step": 1080
    },
    {
      "epoch": 0.6055555555555555,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0003486111111111111,
      "loss": 0.012,
      "step": 1090
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 1.9921875,
      "learning_rate": 0.00034722222222222224,
      "loss": 0.0156,
      "step": 1100
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.05078125,
      "learning_rate": 0.00034583333333333335,
      "loss": 0.0159,
      "step": 1110
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.1396484375,
      "learning_rate": 0.0003444444444444445,
      "loss": 0.0112,
      "step": 1120
    },
    {
      "epoch": 0.6277777777777778,
      "grad_norm": 0.7265625,
      "learning_rate": 0.00034305555555555554,
      "loss": 0.0197,
      "step": 1130
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.1826171875,
      "learning_rate": 0.00034166666666666666,
      "loss": 0.0107,
      "step": 1140
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 1.125,
      "learning_rate": 0.0003402777777777778,
      "loss": 0.0114,
      "step": 1150
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.27734375,
      "learning_rate": 0.0003388888888888889,
      "loss": 0.0154,
      "step": 1160
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.055419921875,
      "learning_rate": 0.0003375,
      "loss": 0.0125,
      "step": 1170
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 1.15625,
      "learning_rate": 0.00033611111111111114,
      "loss": 0.0096,
      "step": 1180
    },
    {
      "epoch": 0.6611111111111111,
      "grad_norm": 0.115234375,
      "learning_rate": 0.0003347222222222222,
      "loss": 0.0147,
      "step": 1190
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.2216796875,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.0257,
      "step": 1200
    },
    {
      "epoch": 0.6722222222222223,
      "grad_norm": 0.244140625,
      "learning_rate": 0.00033194444444444444,
      "loss": 0.0251,
      "step": 1210
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 0.33203125,
      "learning_rate": 0.00033055555555555556,
      "loss": 0.017,
      "step": 1220
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.009521484375,
      "learning_rate": 0.0003291666666666667,
      "loss": 0.011,
      "step": 1230
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.890625,
      "learning_rate": 0.0003277777777777778,
      "loss": 0.0099,
      "step": 1240
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0003263888888888889,
      "loss": 0.0149,
      "step": 1250
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.062255859375,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.0233,
      "step": 1260
    },
    {
      "epoch": 0.7055555555555556,
      "grad_norm": 0.0712890625,
      "learning_rate": 0.00032361111111111116,
      "loss": 0.0124,
      "step": 1270
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.08056640625,
      "learning_rate": 0.0003222222222222222,
      "loss": 0.016,
      "step": 1280
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.205078125,
      "learning_rate": 0.00032083333333333334,
      "loss": 0.0159,
      "step": 1290
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 0.0341796875,
      "learning_rate": 0.0003194444444444444,
      "loss": 0.0158,
      "step": 1300
    },
    {
      "epoch": 0.7277777777777777,
      "grad_norm": 0.9765625,
      "learning_rate": 0.00031805555555555553,
      "loss": 0.0182,
      "step": 1310
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.1865234375,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.003,
      "step": 1320
    },
    {
      "epoch": 0.7388888888888889,
      "grad_norm": 0.07275390625,
      "learning_rate": 0.00031527777777777777,
      "loss": 0.007,
      "step": 1330
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0003138888888888889,
      "loss": 0.0157,
      "step": 1340
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.5078125,
      "learning_rate": 0.0003125,
      "loss": 0.0075,
      "step": 1350
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 2.21875,
      "learning_rate": 0.0003111111111111111,
      "loss": 0.0151,
      "step": 1360
    },
    {
      "epoch": 0.7611111111111111,
      "grad_norm": 0.01007080078125,
      "learning_rate": 0.00030972222222222225,
      "loss": 0.0036,
      "step": 1370
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.86328125,
      "learning_rate": 0.00030833333333333337,
      "loss": 0.0082,
      "step": 1380
    },
    {
      "epoch": 0.7722222222222223,
      "grad_norm": 0.04638671875,
      "learning_rate": 0.0003069444444444445,
      "loss": 0.004,
      "step": 1390
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.080078125,
      "learning_rate": 0.0003055555555555556,
      "loss": 0.0147,
      "step": 1400
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.384765625,
      "learning_rate": 0.00030416666666666667,
      "loss": 0.0208,
      "step": 1410
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0003027777777777778,
      "loss": 0.0196,
      "step": 1420
    },
    {
      "epoch": 0.7944444444444444,
      "grad_norm": 0.2197265625,
      "learning_rate": 0.00030138888888888885,
      "loss": 0.0222,
      "step": 1430
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.26953125,
      "learning_rate": 0.0003,
      "loss": 0.0144,
      "step": 1440
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 0.30859375,
      "learning_rate": 0.0002986111111111111,
      "loss": 0.0068,
      "step": 1450
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 0.035888671875,
      "learning_rate": 0.0002972222222222222,
      "loss": 0.0179,
      "step": 1460
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.003997802734375,
      "learning_rate": 0.00029583333333333333,
      "loss": 0.0076,
      "step": 1470
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.79296875,
      "learning_rate": 0.00029444444444444445,
      "loss": 0.0163,
      "step": 1480
    },
    {
      "epoch": 0.8277777777777777,
      "grad_norm": 0.2421875,
      "learning_rate": 0.00029305555555555557,
      "loss": 0.0055,
      "step": 1490
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.177734375,
      "learning_rate": 0.0002916666666666667,
      "loss": 0.009,
      "step": 1500
    },
    {
      "epoch": 0.8388888888888889,
      "grad_norm": 0.006591796875,
      "learning_rate": 0.0002902777777777778,
      "loss": 0.0064,
      "step": 1510
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.2158203125,
      "learning_rate": 0.0002888888888888889,
      "loss": 0.0064,
      "step": 1520
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.055908203125,
      "learning_rate": 0.0002875,
      "loss": 0.0316,
      "step": 1530
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 0.042236328125,
      "learning_rate": 0.0002861111111111111,
      "loss": 0.0091,
      "step": 1540
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 0.2373046875,
      "learning_rate": 0.00028472222222222223,
      "loss": 0.0186,
      "step": 1550
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.126953125,
      "learning_rate": 0.00028333333333333335,
      "loss": 0.0096,
      "step": 1560
    },
    {
      "epoch": 0.8722222222222222,
      "grad_norm": 0.423828125,
      "learning_rate": 0.0002819444444444445,
      "loss": 0.0142,
      "step": 1570
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 0.24609375,
      "learning_rate": 0.00028055555555555554,
      "loss": 0.0093,
      "step": 1580
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.1298828125,
      "learning_rate": 0.00027916666666666666,
      "loss": 0.0081,
      "step": 1590
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.0625,
      "learning_rate": 0.0002777777777777778,
      "loss": 0.0113,
      "step": 1600
    },
    {
      "epoch": 0.8944444444444445,
      "grad_norm": 0.95703125,
      "learning_rate": 0.0002763888888888889,
      "loss": 0.0177,
      "step": 1610
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.34375,
      "learning_rate": 0.000275,
      "loss": 0.0162,
      "step": 1620
    },
    {
      "epoch": 0.9055555555555556,
      "grad_norm": 0.162109375,
      "learning_rate": 0.00027361111111111114,
      "loss": 0.0171,
      "step": 1630
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.08251953125,
      "learning_rate": 0.0002722222222222222,
      "loss": 0.0088,
      "step": 1640
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.07080078125,
      "learning_rate": 0.0002708333333333333,
      "loss": 0.0129,
      "step": 1650
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.451171875,
      "learning_rate": 0.00026944444444444444,
      "loss": 0.0056,
      "step": 1660
    },
    {
      "epoch": 0.9277777777777778,
      "grad_norm": 0.01397705078125,
      "learning_rate": 0.00026805555555555556,
      "loss": 0.0041,
      "step": 1670
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.1103515625,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.0228,
      "step": 1680
    },
    {
      "epoch": 0.9388888888888889,
      "grad_norm": 0.12255859375,
      "learning_rate": 0.0002652777777777778,
      "loss": 0.007,
      "step": 1690
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 0.515625,
      "learning_rate": 0.0002638888888888889,
      "loss": 0.0223,
      "step": 1700
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.109375,
      "learning_rate": 0.00026250000000000004,
      "loss": 0.0229,
      "step": 1710
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.0091552734375,
      "learning_rate": 0.00026111111111111116,
      "loss": 0.0032,
      "step": 1720
    },
    {
      "epoch": 0.9611111111111111,
      "grad_norm": 0.181640625,
      "learning_rate": 0.0002597222222222222,
      "loss": 0.0128,
      "step": 1730
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.95703125,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.0221,
      "step": 1740
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 0.2421875,
      "learning_rate": 0.0002569444444444444,
      "loss": 0.0085,
      "step": 1750
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.185546875,
      "learning_rate": 0.00025555555555555553,
      "loss": 0.011,
      "step": 1760
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.35546875,
      "learning_rate": 0.00025416666666666665,
      "loss": 0.0067,
      "step": 1770
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 0.453125,
      "learning_rate": 0.00025277777777777777,
      "loss": 0.0049,
      "step": 1780
    },
    {
      "epoch": 0.9944444444444445,
      "grad_norm": 0.0052490234375,
      "learning_rate": 0.0002513888888888889,
      "loss": 0.0081,
      "step": 1790
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.021240234375,
      "learning_rate": 0.00025,
      "loss": 0.0188,
      "step": 1800
    },
    {
      "epoch": 1.0055555555555555,
      "grad_norm": 0.326171875,
      "learning_rate": 0.0002486111111111111,
      "loss": 0.0121,
      "step": 1810
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.43359375,
      "learning_rate": 0.00024722222222222224,
      "loss": 0.0089,
      "step": 1820
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.0037078857421875,
      "learning_rate": 0.0002458333333333333,
      "loss": 0.0087,
      "step": 1830
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.005279541015625,
      "learning_rate": 0.00024444444444444443,
      "loss": 0.0064,
      "step": 1840
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 0.056884765625,
      "learning_rate": 0.00024305555555555555,
      "loss": 0.0049,
      "step": 1850
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.15625,
      "learning_rate": 0.00024166666666666667,
      "loss": 0.0032,
      "step": 1860
    },
    {
      "epoch": 1.038888888888889,
      "grad_norm": 0.404296875,
      "learning_rate": 0.0002402777777777778,
      "loss": 0.0105,
      "step": 1870
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.51171875,
      "learning_rate": 0.0002388888888888889,
      "loss": 0.0038,
      "step": 1880
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.5625,
      "learning_rate": 0.0002375,
      "loss": 0.0039,
      "step": 1890
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 0.059814453125,
      "learning_rate": 0.00023611111111111112,
      "loss": 0.006,
      "step": 1900
    },
    {
      "epoch": 1.0611111111111111,
      "grad_norm": 0.095703125,
      "learning_rate": 0.00023472222222222224,
      "loss": 0.0087,
      "step": 1910
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.02001953125,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.0027,
      "step": 1920
    },
    {
      "epoch": 1.0722222222222222,
      "grad_norm": 0.6875,
      "learning_rate": 0.00023194444444444445,
      "loss": 0.0082,
      "step": 1930
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.2353515625,
      "learning_rate": 0.00023055555555555557,
      "loss": 0.0093,
      "step": 1940
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.109375,
      "learning_rate": 0.00022916666666666666,
      "loss": 0.0032,
      "step": 1950
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.031982421875,
      "learning_rate": 0.00022777777777777778,
      "loss": 0.0068,
      "step": 1960
    },
    {
      "epoch": 1.0944444444444446,
      "grad_norm": 0.01043701171875,
      "learning_rate": 0.0002263888888888889,
      "loss": 0.0054,
      "step": 1970
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.002838134765625,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.007,
      "step": 1980
    },
    {
      "epoch": 1.1055555555555556,
      "grad_norm": 0.00653076171875,
      "learning_rate": 0.0002236111111111111,
      "loss": 0.0135,
      "step": 1990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.00994873046875,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.0033,
      "step": 2000
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.33203125,
      "learning_rate": 0.00022083333333333333,
      "loss": 0.0087,
      "step": 2010
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 0.03759765625,
      "learning_rate": 0.00021944444444444444,
      "loss": 0.0163,
      "step": 2020
    },
    {
      "epoch": 1.1277777777777778,
      "grad_norm": 0.203125,
      "learning_rate": 0.00021805555555555556,
      "loss": 0.0079,
      "step": 2030
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.061279296875,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.0048,
      "step": 2040
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 0.03759765625,
      "learning_rate": 0.0002152777777777778,
      "loss": 0.0013,
      "step": 2050
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.013671875,
      "learning_rate": 0.0002138888888888889,
      "loss": 0.0031,
      "step": 2060
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.0025482177734375,
      "learning_rate": 0.0002125,
      "loss": 0.0019,
      "step": 2070
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.140625,
      "learning_rate": 0.0002111111111111111,
      "loss": 0.0066,
      "step": 2080
    },
    {
      "epoch": 1.1611111111111112,
      "grad_norm": 0.22265625,
      "learning_rate": 0.00020972222222222223,
      "loss": 0.0063,
      "step": 2090
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.19140625,
      "learning_rate": 0.00020833333333333335,
      "loss": 0.0036,
      "step": 2100
    },
    {
      "epoch": 1.1722222222222223,
      "grad_norm": 0.0233154296875,
      "learning_rate": 0.00020694444444444444,
      "loss": 0.002,
      "step": 2110
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.39453125,
      "learning_rate": 0.00020555555555555556,
      "loss": 0.0098,
      "step": 2120
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.007049560546875,
      "learning_rate": 0.00020416666666666668,
      "loss": 0.0042,
      "step": 2130
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 0.00136566162109375,
      "learning_rate": 0.00020277777777777777,
      "loss": 0.0068,
      "step": 2140
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 0.00341796875,
      "learning_rate": 0.0002013888888888889,
      "loss": 0.0052,
      "step": 2150
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.091796875,
      "learning_rate": 0.0002,
      "loss": 0.0065,
      "step": 2160
    },
    {
      "epoch": 1.2055555555555555,
      "grad_norm": 0.08935546875,
      "learning_rate": 0.0001986111111111111,
      "loss": 0.0024,
      "step": 2170
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 0.0279541015625,
      "learning_rate": 0.00019722222222222222,
      "loss": 0.0034,
      "step": 2180
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.0206298828125,
      "learning_rate": 0.00019583333333333334,
      "loss": 0.0037,
      "step": 2190
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.6328125,
      "learning_rate": 0.00019444444444444446,
      "loss": 0.0119,
      "step": 2200
    },
    {
      "epoch": 1.2277777777777779,
      "grad_norm": 0.56640625,
      "learning_rate": 0.00019305555555555558,
      "loss": 0.0076,
      "step": 2210
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.275390625,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.0037,
      "step": 2220
    },
    {
      "epoch": 1.238888888888889,
      "grad_norm": 0.005218505859375,
      "learning_rate": 0.00019027777777777776,
      "loss": 0.0028,
      "step": 2230
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.00799560546875,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.0041,
      "step": 2240
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.029296875,
      "learning_rate": 0.0001875,
      "loss": 0.0051,
      "step": 2250
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.00360107421875,
      "learning_rate": 0.00018611111111111112,
      "loss": 0.0099,
      "step": 2260
    },
    {
      "epoch": 1.261111111111111,
      "grad_norm": 0.064453125,
      "learning_rate": 0.00018472222222222224,
      "loss": 0.0197,
      "step": 2270
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.1494140625,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.0124,
      "step": 2280
    },
    {
      "epoch": 1.2722222222222221,
      "grad_norm": 0.032958984375,
      "learning_rate": 0.00018194444444444443,
      "loss": 0.0037,
      "step": 2290
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 0.0303955078125,
      "learning_rate": 0.00018055555555555555,
      "loss": 0.0046,
      "step": 2300
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.56640625,
      "learning_rate": 0.00017916666666666667,
      "loss": 0.0099,
      "step": 2310
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.291015625,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.0053,
      "step": 2320
    },
    {
      "epoch": 1.2944444444444445,
      "grad_norm": 0.255859375,
      "learning_rate": 0.0001763888888888889,
      "loss": 0.0021,
      "step": 2330
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3828125,
      "learning_rate": 0.000175,
      "loss": 0.0032,
      "step": 2340
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 0.00970458984375,
      "learning_rate": 0.00017361111111111112,
      "loss": 0.004,
      "step": 2350
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.0191650390625,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.0043,
      "step": 2360
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 1.140625,
      "learning_rate": 0.00017083333333333333,
      "loss": 0.0078,
      "step": 2370
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 0.09326171875,
      "learning_rate": 0.00016944444444444445,
      "loss": 0.0049,
      "step": 2380
    },
    {
      "epoch": 1.3277777777777777,
      "grad_norm": 0.006103515625,
      "learning_rate": 0.00016805555555555557,
      "loss": 0.002,
      "step": 2390
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.001800537109375,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.0021,
      "step": 2400
    },
    {
      "epoch": 1.338888888888889,
      "grad_norm": 0.0032501220703125,
      "learning_rate": 0.00016527777777777778,
      "loss": 0.0007,
      "step": 2410
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.00604248046875,
      "learning_rate": 0.0001638888888888889,
      "loss": 0.0034,
      "step": 2420
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.255859375,
      "learning_rate": 0.00016250000000000002,
      "loss": 0.0067,
      "step": 2430
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.031982421875,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.0055,
      "step": 2440
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 0.0458984375,
      "learning_rate": 0.0001597222222222222,
      "loss": 0.0067,
      "step": 2450
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.01416015625,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.0057,
      "step": 2460
    },
    {
      "epoch": 1.3722222222222222,
      "grad_norm": 0.00531005859375,
      "learning_rate": 0.00015694444444444444,
      "loss": 0.0037,
      "step": 2470
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.08837890625,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.0036,
      "step": 2480
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.00421142578125,
      "learning_rate": 0.00015416666666666668,
      "loss": 0.0014,
      "step": 2490
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 0.00106048583984375,
      "learning_rate": 0.0001527777777777778,
      "loss": 0.0021,
      "step": 2500
    },
    {
      "epoch": 1.3944444444444444,
      "grad_norm": 0.002197265625,
      "learning_rate": 0.0001513888888888889,
      "loss": 0.0025,
      "step": 2510
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.0037384033203125,
      "learning_rate": 0.00015,
      "loss": 0.0026,
      "step": 2520
    },
    {
      "epoch": 1.4055555555555554,
      "grad_norm": 0.001373291015625,
      "learning_rate": 0.0001486111111111111,
      "loss": 0.0034,
      "step": 2530
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.388671875,
      "learning_rate": 0.00014722222222222223,
      "loss": 0.0044,
      "step": 2540
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.00518798828125,
      "learning_rate": 0.00014583333333333335,
      "loss": 0.0078,
      "step": 2550
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.310546875,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.0029,
      "step": 2560
    },
    {
      "epoch": 1.4277777777777778,
      "grad_norm": 0.06396484375,
      "learning_rate": 0.00014305555555555556,
      "loss": 0.0022,
      "step": 2570
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.00823974609375,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.0012,
      "step": 2580
    },
    {
      "epoch": 1.4388888888888889,
      "grad_norm": 0.06689453125,
      "learning_rate": 0.00014027777777777777,
      "loss": 0.001,
      "step": 2590
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.64453125,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.009,
      "step": 2600
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.28515625,
      "learning_rate": 0.0001375,
      "loss": 0.0025,
      "step": 2610
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.0184326171875,
      "learning_rate": 0.0001361111111111111,
      "loss": 0.001,
      "step": 2620
    },
    {
      "epoch": 1.4611111111111112,
      "grad_norm": 0.049072265625,
      "learning_rate": 0.00013472222222222222,
      "loss": 0.0086,
      "step": 2630
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.1005859375,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.006,
      "step": 2640
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.00830078125,
      "learning_rate": 0.00013194444444444446,
      "loss": 0.0019,
      "step": 2650
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.11767578125,
      "learning_rate": 0.00013055555555555558,
      "loss": 0.005,
      "step": 2660
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.0244140625,
      "learning_rate": 0.00012916666666666667,
      "loss": 0.0012,
      "step": 2670
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.083984375,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.0025,
      "step": 2680
    },
    {
      "epoch": 1.4944444444444445,
      "grad_norm": 0.103515625,
      "learning_rate": 0.00012638888888888888,
      "loss": 0.0029,
      "step": 2690
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.15625,
      "learning_rate": 0.000125,
      "loss": 0.0073,
      "step": 2700
    },
    {
      "epoch": 1.5055555555555555,
      "grad_norm": 0.0184326171875,
      "learning_rate": 0.00012361111111111112,
      "loss": 0.0026,
      "step": 2710
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.068359375,
      "learning_rate": 0.00012222222222222221,
      "loss": 0.0015,
      "step": 2720
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.037841796875,
      "learning_rate": 0.00012083333333333333,
      "loss": 0.0046,
      "step": 2730
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.0277099609375,
      "learning_rate": 0.00011944444444444445,
      "loss": 0.0028,
      "step": 2740
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.06787109375,
      "learning_rate": 0.00011805555555555556,
      "loss": 0.0027,
      "step": 2750
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.08056640625,
      "learning_rate": 0.00011666666666666667,
      "loss": 0.0034,
      "step": 2760
    },
    {
      "epoch": 1.5388888888888888,
      "grad_norm": 0.384765625,
      "learning_rate": 0.00011527777777777778,
      "loss": 0.009,
      "step": 2770
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 0.025146484375,
      "learning_rate": 0.00011388888888888889,
      "loss": 0.001,
      "step": 2780
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00011250000000000001,
      "loss": 0.0056,
      "step": 2790
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.2119140625,
      "learning_rate": 0.0001111111111111111,
      "loss": 0.0029,
      "step": 2800
    },
    {
      "epoch": 1.5611111111111111,
      "grad_norm": 0.000362396240234375,
      "learning_rate": 0.00010972222222222222,
      "loss": 0.0019,
      "step": 2810
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.0498046875,
      "learning_rate": 0.00010833333333333334,
      "loss": 0.0009,
      "step": 2820
    },
    {
      "epoch": 1.5722222222222222,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00010694444444444445,
      "loss": 0.0049,
      "step": 2830
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.027099609375,
      "learning_rate": 0.00010555555555555555,
      "loss": 0.0021,
      "step": 2840
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.54296875,
      "learning_rate": 0.00010416666666666667,
      "loss": 0.0045,
      "step": 2850
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 0.267578125,
      "learning_rate": 0.00010277777777777778,
      "loss": 0.0012,
      "step": 2860
    },
    {
      "epoch": 1.5944444444444446,
      "grad_norm": 0.080078125,
      "learning_rate": 0.00010138888888888889,
      "loss": 0.0058,
      "step": 2870
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.0224609375,
      "learning_rate": 0.0001,
      "loss": 0.0032,
      "step": 2880
    },
    {
      "epoch": 1.6055555555555556,
      "grad_norm": 0.1318359375,
      "learning_rate": 9.861111111111111e-05,
      "loss": 0.0042,
      "step": 2890
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 0.000896453857421875,
      "learning_rate": 9.722222222222223e-05,
      "loss": 0.0039,
      "step": 2900
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.021484375,
      "learning_rate": 9.583333333333334e-05,
      "loss": 0.0082,
      "step": 2910
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.06494140625,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.0048,
      "step": 2920
    },
    {
      "epoch": 1.6277777777777778,
      "grad_norm": 0.11865234375,
      "learning_rate": 9.305555555555556e-05,
      "loss": 0.0044,
      "step": 2930
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.000690460205078125,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.0009,
      "step": 2940
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 0.019287109375,
      "learning_rate": 9.027777777777777e-05,
      "loss": 0.0013,
      "step": 2950
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 0.48828125,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.003,
      "step": 2960
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.26953125,
      "learning_rate": 8.75e-05,
      "loss": 0.0048,
      "step": 2970
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.031494140625,
      "learning_rate": 8.611111111111112e-05,
      "loss": 0.0013,
      "step": 2980
    },
    {
      "epoch": 1.661111111111111,
      "grad_norm": 0.033203125,
      "learning_rate": 8.472222222222222e-05,
      "loss": 0.0051,
      "step": 2990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.0010223388671875,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.0052,
      "step": 3000
    },
    {
      "epoch": 1.6722222222222223,
      "grad_norm": 0.00396728515625,
      "learning_rate": 8.194444444444445e-05,
      "loss": 0.0017,
      "step": 3010
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.0673828125,
      "learning_rate": 8.055555555555556e-05,
      "loss": 0.0092,
      "step": 3020
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.04052734375,
      "learning_rate": 7.916666666666666e-05,
      "loss": 0.0009,
      "step": 3030
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 0.306640625,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.0025,
      "step": 3040
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.0184326171875,
      "learning_rate": 7.63888888888889e-05,
      "loss": 0.0062,
      "step": 3050
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.1982421875,
      "learning_rate": 7.5e-05,
      "loss": 0.0011,
      "step": 3060
    },
    {
      "epoch": 1.7055555555555557,
      "grad_norm": 0.001983642578125,
      "learning_rate": 7.361111111111111e-05,
      "loss": 0.0057,
      "step": 3070
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.00830078125,
      "learning_rate": 7.222222222222222e-05,
      "loss": 0.0022,
      "step": 3080
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.00732421875,
      "learning_rate": 7.083333333333334e-05,
      "loss": 0.0027,
      "step": 3090
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 0.1396484375,
      "learning_rate": 6.944444444444444e-05,
      "loss": 0.0032,
      "step": 3100
    },
    {
      "epoch": 1.7277777777777779,
      "grad_norm": 0.365234375,
      "learning_rate": 6.805555555555555e-05,
      "loss": 0.0013,
      "step": 3110
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.0478515625,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0056,
      "step": 3120
    },
    {
      "epoch": 1.738888888888889,
      "grad_norm": 0.01171875,
      "learning_rate": 6.527777777777779e-05,
      "loss": 0.0048,
      "step": 3130
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 0.007476806640625,
      "learning_rate": 6.388888888888888e-05,
      "loss": 0.0039,
      "step": 3140
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.01611328125,
      "learning_rate": 6.25e-05,
      "loss": 0.0013,
      "step": 3150
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.453125,
      "learning_rate": 6.111111111111111e-05,
      "loss": 0.0032,
      "step": 3160
    },
    {
      "epoch": 1.761111111111111,
      "grad_norm": 0.01043701171875,
      "learning_rate": 5.972222222222223e-05,
      "loss": 0.0025,
      "step": 3170
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.06640625,
      "learning_rate": 5.833333333333333e-05,
      "loss": 0.0042,
      "step": 3180
    },
    {
      "epoch": 1.7722222222222221,
      "grad_norm": 0.109375,
      "learning_rate": 5.6944444444444445e-05,
      "loss": 0.0063,
      "step": 3190
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.003173828125,
      "learning_rate": 5.555555555555555e-05,
      "loss": 0.002,
      "step": 3200
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.1044921875,
      "learning_rate": 5.416666666666667e-05,
      "loss": 0.005,
      "step": 3210
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 0.06298828125,
      "learning_rate": 5.277777777777778e-05,
      "loss": 0.0099,
      "step": 3220
    },
    {
      "epoch": 1.7944444444444443,
      "grad_norm": 0.357421875,
      "learning_rate": 5.138888888888889e-05,
      "loss": 0.0093,
      "step": 3230
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.002593994140625,
      "learning_rate": 5e-05,
      "loss": 0.0078,
      "step": 3240
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.0029449462890625,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 0.0073,
      "step": 3250
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.0703125,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.0006,
      "step": 3260
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.003753662109375,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0039,
      "step": 3270
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.031494140625,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0067,
      "step": 3280
    },
    {
      "epoch": 1.8277777777777777,
      "grad_norm": 0.28125,
      "learning_rate": 4.305555555555556e-05,
      "loss": 0.0028,
      "step": 3290
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.17578125,
      "learning_rate": 4.1666666666666665e-05,
      "loss": 0.0046,
      "step": 3300
    },
    {
      "epoch": 1.838888888888889,
      "grad_norm": 0.0289306640625,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.0013,
      "step": 3310
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 1.03125,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0073,
      "step": 3320
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.10791015625,
      "learning_rate": 3.75e-05,
      "loss": 0.0035,
      "step": 3330
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.052734375,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.0008,
      "step": 3340
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 0.0018768310546875,
      "learning_rate": 3.472222222222222e-05,
      "loss": 0.0007,
      "step": 3350
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.01220703125,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0015,
      "step": 3360
    },
    {
      "epoch": 1.8722222222222222,
      "grad_norm": 0.005889892578125,
      "learning_rate": 3.194444444444444e-05,
      "loss": 0.0019,
      "step": 3370
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.049072265625,
      "learning_rate": 3.0555555555555554e-05,
      "loss": 0.0056,
      "step": 3380
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.042724609375,
      "learning_rate": 2.9166666666666666e-05,
      "loss": 0.0037,
      "step": 3390
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.0198974609375,
      "learning_rate": 2.7777777777777776e-05,
      "loss": 0.007,
      "step": 3400
    },
    {
      "epoch": 1.8944444444444444,
      "grad_norm": 0.6796875,
      "learning_rate": 2.638888888888889e-05,
      "loss": 0.0042,
      "step": 3410
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.03173828125,
      "learning_rate": 2.5e-05,
      "loss": 0.0054,
      "step": 3420
    },
    {
      "epoch": 1.9055555555555554,
      "grad_norm": 0.0101318359375,
      "learning_rate": 2.361111111111111e-05,
      "loss": 0.0047,
      "step": 3430
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.205078125,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0019,
      "step": 3440
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.1171875,
      "learning_rate": 2.0833333333333333e-05,
      "loss": 0.0015,
      "step": 3450
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.349609375,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.0018,
      "step": 3460
    },
    {
      "epoch": 1.9277777777777778,
      "grad_norm": 0.2041015625,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 0.0045,
      "step": 3470
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.1572265625,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0138,
      "step": 3480
    },
    {
      "epoch": 1.9388888888888889,
      "grad_norm": 0.08935546875,
      "learning_rate": 1.5277777777777777e-05,
      "loss": 0.009,
      "step": 3490
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 0.01275634765625,
      "learning_rate": 1.3888888888888888e-05,
      "loss": 0.0013,
      "step": 3500
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.04736328125,
      "learning_rate": 1.25e-05,
      "loss": 0.0015,
      "step": 3510
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.002960205078125,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0086,
      "step": 3520
    },
    {
      "epoch": 1.9611111111111112,
      "grad_norm": 0.00823974609375,
      "learning_rate": 9.722222222222223e-06,
      "loss": 0.0018,
      "step": 3530
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.020263671875,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0012,
      "step": 3540
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.00799560546875,
      "learning_rate": 6.944444444444444e-06,
      "loss": 0.0012,
      "step": 3550
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.0634765625,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.0066,
      "step": 3560
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 1.046875,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0132,
      "step": 3570
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 0.00164031982421875,
      "learning_rate": 2.777777777777778e-06,
      "loss": 0.001,
      "step": 3580
    },
    {
      "epoch": 1.9944444444444445,
      "grad_norm": 0.045654296875,
      "learning_rate": 1.388888888888889e-06,
      "loss": 0.0039,
      "step": 3590
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.09375,
      "learning_rate": 0.0,
      "loss": 0.0035,
      "step": 3600
    }
  ],
  "logging_steps": 10,
  "max_steps": 3600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "total_flos": 2.110621645331497e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
