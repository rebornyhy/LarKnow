{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a414a95-4641-4b24-9e38-6abd19ec53b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bad302-5a2a-4743-8c3e-ad76c5b37038",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths_tem2 = ['../../{}/seed_qa/1hop_{}_beforegen.jsonl',\n",
    "                 '../../{}/seed_qa/2hop_{}_beforegen.jsonl']\n",
    "file_paths_tem1 = ['../../QiJiaoSheng/genqa_from_seed/1hop_qa.jsonl',\n",
    "                 '../../{}/genqa_from_seed/2hop_qa.jsonl',\n",
    "                 '../../{}/genqa_from_seed/2hop_muit_alone.jsonl',\n",
    "                 '../../{}/genqa_from_seed/1hop_alone.jsonl',\n",
    "                 '../../{}/genqa_from_seed/1hop_muit.jsonl']\n",
    "formula_list = [['QiJiaoSheng','qsc'],['SheXiangBao','sbw'],['SiMiaoFang','smf'],['XiaYuXue','xyx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "260e67fc-1962-47c3-aca2-294fc5513229",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = set()\n",
    "for tem in file_paths_tem2:\n",
    "    for form in formula_list:\n",
    "        file_paths.add(tem.format(*form))\n",
    "for tem in file_paths_tem1:\n",
    "    for form in formula_list:\n",
    "        file_paths.add(tem.format(form[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f29e60b-1a9e-461c-bd74-2f7c48fdc80b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths = ['../../QiJiaoSheng/genqa_from_seed/1hop_qa.jsonl',\n",
    " '../../QiJiaoSheng/genqa_from_seed/2hop_muit_alone.jsonl',\n",
    " '../../QiJiaoSheng/genqa_from_seed/2hop_qa.jsonl',\n",
    " '../../QiJiaoSheng/seed_qa/1hop_qsc_beforegen.jsonl',\n",
    " '../../QiJiaoSheng/seed_qa/2hop_qsc_beforegen.jsonl',\n",
    " '../../SheXiangBao/genqa_from_seed/1hop_alone.jsonl',\n",
    " '../../SheXiangBao/genqa_from_seed/1hop_muit.jsonl',\n",
    " '../../SheXiangBao/genqa_from_seed/2hop_muit_alone.jsonl',\n",
    " '../../SheXiangBao/genqa_from_seed/2hop_qa.jsonl',\n",
    " '../../SheXiangBao/seed_qa/1hop_sbw_beforegen.jsonl',\n",
    " '../../SheXiangBao/seed_qa/2hop_sbw_beforegen.jsonl',\n",
    " '../../SiMiaoFang/genqa_from_seed/1hop_alone.jsonl',\n",
    " '../../SiMiaoFang/genqa_from_seed/1hop_muit.jsonl',\n",
    " '../../SiMiaoFang/genqa_from_seed/2hop_muit_alone.jsonl',\n",
    " '../../SiMiaoFang/genqa_from_seed/2hop_qa.jsonl',\n",
    " '../../SiMiaoFang/seed_qa/1hop_smf_beforegen.jsonl',\n",
    " '../../SiMiaoFang/seed_qa/2hop_smf_beforegen.jsonl',\n",
    " '../../XiaYuXue/genqa_from_seed/1hop_alone.jsonl',\n",
    " '../../XiaYuXue/genqa_from_seed/1hop_muit.jsonl',\n",
    " '../../XiaYuXue/genqa_from_seed/2hop_muit_alone.jsonl',\n",
    " '../../XiaYuXue/genqa_from_seed/2hop_qa.jsonl',\n",
    " '../../XiaYuXue/seed_qa/1hop_xyx_beforegen.jsonl',\n",
    " '../../XiaYuXue/seed_qa/2hop_xyx_beforegen.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad432288-dcd9-4000-8436-3a2f17f7623a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_head_tail(ans):\n",
    "    \n",
    "    #获取一系列三元组中的头节点集与尾结点集\n",
    "    #输入为check_format好的一组三元组，三元组之间;隔开\n",
    "    #返回head列表与tail列表\n",
    "    \n",
    "    objset = set()\n",
    "    trilist = ans.split(';')\n",
    "    htlist = []\n",
    "    for tri in trilist:\n",
    "        trii = tri[1:-1]\n",
    "        #print(trii)\n",
    "        triilist = trii.split(',')\n",
    "        #print(triilist)\n",
    "        objset.add(triilist[0])\n",
    "        objset.add(triilist[2])\n",
    "        htlist.append([triilist[0],triilist[2]])\n",
    "    ishead = dict()\n",
    "    istail = dict()\n",
    "    for obj in objset:\n",
    "        ishead[obj] = True\n",
    "        istail[obj] = True\n",
    "    for ht in htlist:\n",
    "        ishead[ht[1]] = False\n",
    "        istail[ht[0]] = False\n",
    "    head = []\n",
    "    tail = []\n",
    "    for key in ishead:\n",
    "        if ishead[key] == True:\n",
    "            head.append(key)\n",
    "    for key in istail:\n",
    "        if istail[key] == True:\n",
    "            tail.append(key)\n",
    "    #print(tail)\n",
    "    return head,tail\n",
    "\n",
    "def get_each_hop_en(ans):\n",
    "    head, tail = get_head_tail(ans)\n",
    "    trilist = ans.split(';')\n",
    "    htlist = []\n",
    "    answer = []\n",
    "    for tri in trilist:\n",
    "        trii = tri[1:-1]\n",
    "        #print(trii)\n",
    "        triilist = trii.split(',')\n",
    "        htlist.append([triilist[0],triilist[1],triilist[2]])\n",
    "    cur_hop_list = head\n",
    "    while len(cur_hop_list) != 0:\n",
    "        nowlist = []\n",
    "        for tris in htlist:\n",
    "            if tris[0] in cur_hop_list:\n",
    "                nowlist.append(tris[2])\n",
    "        answer.append(cur_hop_list)    \n",
    "        cur_hop_list = nowlist\n",
    "        #print(cur_hop_list)\n",
    "    return answer, htlist\n",
    "\n",
    "def qa_isvalid_otherpro(qa_json):\n",
    "    \"\"\"\n",
    "    合理性判断\n",
    "    要求问题中只能有一条关系路径\n",
    "    \"\"\"\n",
    "    each_hop, trilist = get_each_hop_en(qa_json['回答'])\n",
    "    #print(each_hop, trilist)\n",
    "    ques = qa_json['问题']\n",
    "    each_hop_bool = []\n",
    "    rel_list = []\n",
    "    true_en = set()\n",
    "    for hop_en in each_hop:\n",
    "        for en in hop_en:\n",
    "            for tris in trilist:\n",
    "                if tris[0] == en:\n",
    "                    rel_list.append(tris[1])\n",
    "                    break\n",
    "            break\n",
    "    for hop_en in each_hop:\n",
    "        cur_bool = []\n",
    "        for en in hop_en:\n",
    "            if en in ques:\n",
    "                cur_bool.append(True)      \n",
    "                true_en.add(en)\n",
    "            else:\n",
    "                cur_bool.append(False)\n",
    "        each_hop_bool.append(cur_bool) \n",
    "    for formula in ['四妙方','麝香保心丸','下淤血汤','芪胶升白胶囊']:\n",
    "        if formula in ques:\n",
    "            true_en.add(formula)\n",
    "    for idx, hop_bool in enumerate(each_hop_bool):\n",
    "        each_hop_bool[idx] = all(hop_bool)\n",
    "    #print(each_hop_bool)\n",
    "    if any(each_hop_bool[:-1]) == False and each_hop_bool[-1] == True:\n",
    "        qa_json['ques_en'] = each_hop[-1]\n",
    "        qa_json['ans_en'] = each_hop[0]\n",
    "        qa_json['rel_path'] = [rel_list]\n",
    "        qa_json['entity'] = list(true_en)\n",
    "        qa_json['distur'] = dict()\n",
    "        for ent in qa_json['ques_en']:\n",
    "            qa_json['distur'][ent] = qa_json['rel_path']\n",
    "        qa_json['hop'] = len(qa_json['rel_path'][0])\n",
    "        if len(each_hop) != qa_json['hop'] + 1:\n",
    "            return False\n",
    "        qa_json['回答'] = [qa_json['回答']]\n",
    "        return qa_json\n",
    "    if any(each_hop_bool[:-1]) == True:\n",
    "        for idx in range(len(each_hop_bool)-2, -1, -1):\n",
    "            if each_hop_bool[idx] == True:\n",
    "                qa_json['ques_en'] = each_hop[idx]\n",
    "                qa_json['ans_en'] = each_hop[-1]\n",
    "                qa_json['rel_path'] = [rel_list[idx:]]\n",
    "                qa_json['entity'] = list(true_en)\n",
    "                qa_json['distur'] = dict()\n",
    "                for ent in qa_json['ques_en']:\n",
    "                    qa_json['distur'][ent] = qa_json['rel_path']\n",
    "                qa_json['hop'] = len(qa_json['rel_path'][0])\n",
    "                if len(each_hop) != qa_json['hop'] + 1:\n",
    "                    return False\n",
    "                if idx != 0:\n",
    "                    new_trilist = []\n",
    "                    no_en = []\n",
    "                    for enslist in each_hop[0:idx]:\n",
    "                        for ens in enslist:\n",
    "                            no_en.append(ens)\n",
    "                    for tris in trilist:\n",
    "                        if tris[0] not in no_en:\n",
    "                            new_trilist.append('({},{},{})'.format(tris[0], tris[1], tris[2]))\n",
    "                    qa_json['回答'] = ';'.join(new_trilist)\n",
    "                    qa_json['回答'] = [qa_json['回答']]\n",
    "                else:\n",
    "                    qa_json['回答'] = [qa_json['回答']]\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        return qa_json\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1b878-14ff-4691-8dab-bfe169310b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splithop(file_list, save_list):\n",
    "    id1 = 0\n",
    "    id2 = 0\n",
    "    for file_path in file_list:\n",
    "        cur_dataset = jsonlines.open(file_path)\n",
    "        for cur_data in cur_dataset:\n",
    "            cur_data['hop'] = len(cur_data['rel_path'][0])\n",
    "            if cur_data['hop'] == 1:\n",
    "                cur_data['id'] = id1\n",
    "                id1 += 1\n",
    "                save = open(save_list[0], 'a',encoding='utf-8')\n",
    "                save.write(json.dumps(cur_data, ensure_ascii=False) + '\\n')\n",
    "                save.close()\n",
    "            else:\n",
    "                cur_data['id'] = id2\n",
    "                id2 += 1\n",
    "                save = open(save_list[1], 'a',encoding='utf-8')\n",
    "                save.write(json.dumps(cur_data, ensure_ascii=False) + '\\n')\n",
    "                save.close()\n",
    "        cur_dataset.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7133bb92-b329-443d-956b-3931eb09e295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splithop(file_paths, ['./1hop_qa.jsonl','./2hop_qa.jsonl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05357aa9-68dd-4b32-97f4-0a67728fcde7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167\n",
      "239\n",
      "67\n",
      "137\n",
      "8\n",
      "389\n",
      "863\n",
      "344\n",
      "89\n",
      "131\n",
      "10\n",
      "725\n",
      "1484\n",
      "325\n",
      "84\n",
      "221\n",
      "10\n",
      "202\n",
      "409\n",
      "80\n",
      "24\n",
      "63\n",
      "5\n",
      "7076\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for file_path in file_paths:\n",
    "    cur_dataset = jsonlines.open(file_path)\n",
    "    i = 0\n",
    "    for cur_data in cur_dataset:\n",
    "        i+=1\n",
    "    sum+=i\n",
    "    print(i)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095319e4-29b0-44b2-b618-a23dd6dd1936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_graph(graph: list) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    for triplet in graph:\n",
    "        h, r, t = triplet\n",
    "        G.add_edge(h, t, relation=r.strip())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3bca49-cdd0-48a8-a4ce-d2820a25686f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_truth_paths(q_entity: list, a_entity: list, graph: nx.Graph) -> list:\n",
    "    '''\n",
    "    Get shortest paths connecting question and answer entities.\n",
    "    '''\n",
    "    # Select paths\n",
    "    paths = []\n",
    "    for h in q_entity:\n",
    "        if h not in graph:\n",
    "            continue\n",
    "        for t in a_entity:\n",
    "            if t not in graph:\n",
    "                continue\n",
    "            try:\n",
    "                for p in nx.all_shortest_paths(graph, h, t):\n",
    "                    paths.append(p)\n",
    "            except:\n",
    "                pass\n",
    "    # Add relation to paths\n",
    "    result_paths = []\n",
    "    for p in paths:\n",
    "        tmp = []\n",
    "        for i in range(len(p)-1):\n",
    "            u = p[i]\n",
    "            v = p[i+1]\n",
    "            tmp.append((u, graph[u][v]['relation'], v))\n",
    "        result_paths.append(tmp)\n",
    "    pathset = set()\n",
    "    for cur in result_paths:\n",
    "        cur_path = []\n",
    "        for tri in cur:\n",
    "            cur_path.append(tri[1])\n",
    "        pathset.add('-'.join(cur_path))\n",
    "    answer = []\n",
    "    for singpath in pathset:\n",
    "        answer.append(singpath.split('-'))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4204716-cd40-45dc-93cc-0e634876fa45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = [[\"四妙方\", \"草药\", \"黄柏\"], [\"四妙方\", \"草药\", \"苍术\"], [\"四妙方\", \"草药\", \"薏苡仁\"], [\"四妙方\", \"草药\", \"牛膝\"], [\"四妙方\", \"草药\", \"苍术\"], [\"四妙方\", \"疾病\", \"非酒精性脂肪性肝病\"], [\"四妙方\", \"症候\", \"痛风痹症\"], [\"四妙方\", \"症状\", \"足膝红肿疼痛\"], [\"四妙方\", \"症候\", \"湿热下注\"], [\"四妙方\", \"症候\", \"湿热痿证\"], [\"四妙方\", \"疾病\", \"清热燥湿\"], [\"黄柏\", \"效用\", \"清热燥湿\"], [\"牛膝\", \"效用\", \"肝肾\"], [\"牛膝\", \"效用\", \"强筋骨\"], [\"苍术\", \"效用\", \"燥湿健脾\"], [\"薏苡仁\", \"效用\", \"祛湿热\"], [\"薏苡仁\", \"效用\", \"利筋络\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ded5d46c-f881-4995-be72-92bd3dc8fa03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = build_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5741625-3592-4a22-a8fa-7453a36096c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['症候'], ['疾病'], ['症状'], ['草药', '效用']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_truth_paths([\"四妙方\"], [\"肝肾\", \"利筋络\", \"清热燥湿\", \"非酒精性脂肪性肝病\", \"强筋骨\", \"痛风痹症\", \"足膝红肿疼痛\", \"湿热下注\", \"燥湿健脾\", \"祛湿热\", \"湿热痿证\"], graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3034ba83-fecd-4e4b-afde-f2cffdad97de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_from_raw(raw_file, save_list):\n",
    "    id1 = 0\n",
    "    id2 = 0\n",
    "    cur_dataset = jsonlines.open(raw_file)\n",
    "    for cur_data in cur_dataset:\n",
    "        cur_g = build_graph(cur_data[\"graph\"])\n",
    "        rel_path = get_truth_paths(cur_data[\"q_entity\"], cur_data[\"a_entity\"], cur_g)\n",
    "        if len(rel_path) == 1:\n",
    "            if '效用' in rel_path[0]:\n",
    "                newline = dict()\n",
    "                tri_list = cur_data[\"graph\"]\n",
    "                new_list = []\n",
    "                for tri in tri_list:\n",
    "                    for idx, en in enumerate(tri):\n",
    "                        ent = en.replace(';','；')\n",
    "                        tri[idx] = ent.replace(',','，')\n",
    "                    new_list.append('({},{},{})'.format(tri[0], tri[1], tri[2]))\n",
    "                newline['问题'] = cur_data[\"question\"]\n",
    "                newline['回答']  = ';'.join(new_list)\n",
    "                check = qa_isvalid_otherpro(newline)\n",
    "                if check != False:\n",
    "                    if check['hop'] == 1:\n",
    "                        check['id'] = id1\n",
    "                        id1 += 1\n",
    "                        save = open(save_list[0], 'a',encoding='utf-8')\n",
    "                        save.write(json.dumps(check, ensure_ascii=False) + '\\n')\n",
    "                        save.close()\n",
    "                    else:\n",
    "                        check['id'] = id2\n",
    "                        id2 += 1\n",
    "                        save = open(save_list[1], 'a',encoding='utf-8')\n",
    "                        save.write(json.dumps(check, ensure_ascii=False) + '\\n')\n",
    "                        save.close()\n",
    "    cur_dataset.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182b05f3-f33f-4853-b3df-c2fca292f43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_from_raw('./raw_dataset.jsonl', ['./raw_1hop_qa.jsonl','./raw_2hop_qa.jsonl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b437892e-07b4-4942-8a47-b3a9b49f44f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listequal(list1, list2):\n",
    "    from collections import Counter\n",
    "    are_lists_equal = (Counter(list1) == Counter(list2))\n",
    "    return are_lists_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf72830-be3a-4a74-821e-a98dac51df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def combine_1hop(qa_file, num, save_file):\n",
    "    cur_dataset = jsonlines.open(qa_file)\n",
    "    cur_dataset = list(cur_dataset)\n",
    "    index = [i for i in range(len(cur_dataset))]\n",
    "    sample_set = set()\n",
    "    idx = 0\n",
    "    while len(sample_set) < num:\n",
    "        cur_sam = random.sample(index, 2)\n",
    "        signal = '-'.join([str(cur_sam[0]),str(cur_sam[1])])\n",
    "        if signal in sample_set:\n",
    "            continue\n",
    "        qa1 = cur_dataset[cur_sam[0]]\n",
    "        qa2 = cur_dataset[cur_sam[1]]\n",
    "        ans_en1 = ','.join(qa1[\"ans_en\"])\n",
    "        ans_en2 = ','.join(qa2[\"ans_en\"])\n",
    "        qa1_key = []\n",
    "        qa2_key = []\n",
    "        for key, value in qa1['distur'].items():\n",
    "            qa1_key.append(key)\n",
    "        for key, value in qa2['distur'].items():\n",
    "            qa2_key.append(key)\n",
    "        if listequal(qa1_key, qa2_key) and listequal(qa1['rel_path'][0], qa2['rel_path'][0]):\n",
    "            continue\n",
    "        if ans_en1 in ans_en2 or ans_en2 in ans_en1:\n",
    "            continue\n",
    "        sample_set.add(signal)\n",
    "        new_distur = dict()\n",
    "        for key, value in qa1['distur'].items():\n",
    "            getval = new_distur.get(key, None)\n",
    "            if getval == None:\n",
    "                new_distur[key] = []\n",
    "            for sin_path in value:\n",
    "                if sin_path not in new_distur[key]:\n",
    "                    new_distur[key].append(sin_path)\n",
    "        for key, value in qa2['distur'].items():\n",
    "            getval = new_distur.get(key, None)\n",
    "            if getval == None:\n",
    "                new_distur[key] = []\n",
    "            for sin_path in value:\n",
    "                if sin_path not in new_distur[key]:\n",
    "                    new_distur[key].append(sin_path)\n",
    "        while qa1['问题'][-1] == '；':\n",
    "            qa1['问题'] = qa1['问题'][:-1]\n",
    "        while qa2['问题'][-1] == '；':\n",
    "            qa2['问题'] = qa2['问题'][:-1]   \n",
    "        qa1['问题'] = qa1['问题'].replace('?','？')\n",
    "        qa2['问题'] = qa2['问题'].replace('?','？')\n",
    "        newline = dict()\n",
    "        newline['问题'] = qa1['问题'] + qa2['问题']\n",
    "        newline['回答'] = qa1['回答'] + qa2['回答']\n",
    "        newline['ques_en'] = list(set(qa1['ques_en'] + qa2['ques_en']))\n",
    "        newline['ans_en'] = list(set(qa1['ans_en'] + qa2['ans_en']))\n",
    "        new_rel_path = []\n",
    "        for p in qa1['rel_path']:\n",
    "            if p not in new_rel_path:\n",
    "                new_rel_path.append(p)\n",
    "        for p in qa2['rel_path']:\n",
    "            if p not in new_rel_path:\n",
    "                new_rel_path.append(p)\n",
    "        newline['rel_path'] = new_rel_path\n",
    "        newline[\"entity\"] = list(set(qa1['entity'] + qa2['entity']))\n",
    "        newline[\"distur\"] = new_distur\n",
    "        newline['hop'] = max(qa1['hop'],qa2['hop'])\n",
    "        newline['class'] = 'combine1_' + signal\n",
    "        newline[\"strategy\"]= \"undetermined\"\n",
    "        newline['id'] = idx\n",
    "        idx+=1\n",
    "        save = open(save_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(newline, ensure_ascii=False) + '\\n')\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b48fbe99-1274-47bf-9a57-175dae2c5b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_1hop('./1hop_qa.jsonl', 1182, './1hop_com.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c721c21d-f02c-42bc-9021-1ceecc55ae33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def combine_2hop(qa_file1, qa_file2, num, save_file):\n",
    "    cur_dataset1 = jsonlines.open(qa_file1)\n",
    "    cur_dataset1 = list(cur_dataset1)\n",
    "    index1 = [i for i in range(len(cur_dataset1))]\n",
    "    cur_dataset2 = jsonlines.open(qa_file2)\n",
    "    cur_dataset2 = list(cur_dataset2)\n",
    "    index2 = [i for i in range(len(cur_dataset2))]\n",
    "    sample_set = set()\n",
    "    idx = 0\n",
    "    while len(sample_set) < num:\n",
    "        cur_sam1 = random.sample(index1, 1)\n",
    "        cur_sam2 = random.sample(index2, 1)\n",
    "        signal = '-'.join([str(cur_sam1[0]),str(cur_sam2[0])])\n",
    "        if signal in sample_set:\n",
    "            continue\n",
    "        qa1 = cur_dataset1[cur_sam1[0]]\n",
    "        qa2 = cur_dataset2[cur_sam2[0]]\n",
    "        ans_en1 = ','.join(qa1[\"ans_en\"])\n",
    "        ans_en2 = ','.join(qa2[\"ans_en\"])\n",
    "        qa1_key = []\n",
    "        qa2_key = []\n",
    "        for key, value in qa1['distur'].items():\n",
    "            qa1_key.append(key)\n",
    "        for key, value in qa2['distur'].items():\n",
    "            qa2_key.append(key)\n",
    "        if listequal(qa1_key, qa2_key) and listequal(qa1['rel_path'][0], qa2['rel_path'][0]):\n",
    "            continue\n",
    "        if ans_en1 in ans_en2 or ans_en2 in ans_en1:\n",
    "            continue\n",
    "        sample_set.add(signal)\n",
    "        new_distur = dict()\n",
    "        for key, value in qa1['distur'].items():\n",
    "            getval = new_distur.get(key, None)\n",
    "            if getval == None:\n",
    "                new_distur[key] = []\n",
    "            for sin_path in value:\n",
    "                if sin_path not in new_distur[key]:\n",
    "                    new_distur[key].append(sin_path)\n",
    "        for key, value in qa2['distur'].items():\n",
    "            getval = new_distur.get(key, None)\n",
    "            if getval == None:\n",
    "                new_distur[key] = []\n",
    "            for sin_path in value:\n",
    "                if sin_path not in new_distur[key]:\n",
    "                    new_distur[key].append(sin_path)\n",
    "        while qa1['问题'][-1] == '；':\n",
    "            qa1['问题'] = qa1['问题'][:-1]\n",
    "        while qa2['问题'][-1] == '；':\n",
    "            qa2['问题'] = qa2['问题'][:-1]   \n",
    "        qa1['问题'] = qa1['问题'].replace('?','？')\n",
    "        qa2['问题'] = qa2['问题'].replace('?','？')\n",
    "        newline = dict()\n",
    "        newline['问题'] = qa1['问题'] + qa2['问题']\n",
    "        newline['回答'] = qa1['回答'] + qa2['回答']\n",
    "        newline['ques_en'] = list(set(qa1['ques_en'] + qa2['ques_en']))\n",
    "        newline['ans_en'] = list(set(qa1['ans_en'] + qa2['ans_en']))\n",
    "        new_rel_path = []\n",
    "        for p in qa1['rel_path']:\n",
    "            if p not in new_rel_path:\n",
    "                new_rel_path.append(p)\n",
    "        for p in qa2['rel_path']:\n",
    "            if p not in new_rel_path:\n",
    "                new_rel_path.append(p)\n",
    "        newline['rel_path'] = new_rel_path\n",
    "        newline[\"entity\"] = list(set(qa1['entity'] + qa2['entity']))\n",
    "        newline[\"distur\"] = new_distur\n",
    "        newline['hop'] = max(qa1['hop'],qa2['hop'])\n",
    "        newline['class'] = 'combine1_' + signal\n",
    "        newline[\"strategy\"]= \"undetermined\"\n",
    "        newline['id'] = idx\n",
    "        idx+=1\n",
    "        save = open(save_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(newline, ensure_ascii=False) + '\\n')\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b27d389d-6445-4ad1-b600-710cc0389a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_2hop('./1hop_qa.jsonl', './2hop_qa.jsonl', 1181, './2hop_com.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba2a42e-386a-4062-a1ca-30c9892186fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "want_num = 464\n",
    "idx = 7036\n",
    "cur_dataset = jsonlines.open('./raw_1hop_qa.jsonl')\n",
    "cur_dataset = list(cur_dataset)\n",
    "sam = random.sample(cur_dataset, want_num)\n",
    "save = open('./1hop_qa.jsonl', 'a',encoding='utf-8')\n",
    "for sam_s in sam:\n",
    "    sam_s['class'] = 'samfromraw_' + str(sam_s['id'])\n",
    "    sam_s['id'] = idx\n",
    "    idx += 1\n",
    "    sam_s[\"strategy\"] = 'undetermined'\n",
    "    save.write(json.dumps(sam_s, ensure_ascii=False) + '\\n')\n",
    "save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d04a93-1200-4088-849a-d2c48d045fb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 40\n",
    "cur_dataset = jsonlines.open('./raw_2hop_qa.jsonl')\n",
    "cur_dataset = list(cur_dataset)\n",
    "save = open('./2hop_qa.jsonl', 'a',encoding='utf-8')\n",
    "for sam_s in cur_dataset:\n",
    "    sam_s['class'] = 'samfromraw_' + str(sam_s['id'])\n",
    "    sam_s['id'] = idx\n",
    "    idx += 1\n",
    "    sam_s[\"strategy\"] = 'undetermined'\n",
    "    save.write(json.dumps(sam_s, ensure_ascii=False) + '\\n')\n",
    "save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e885bec-5caa-4ff0-a936-0a131b1650d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a'] in [['a'],['b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab378d2a-e269-421b-ba42-b9fad848b624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(['a','a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e6efe4-852a-4c70-bf8d-3fd499c09a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_check_shuffle_split(file_pathlist, split_, train_file, test_file):\n",
    "    all_data = []\n",
    "    for file_path in file_pathlist:\n",
    "        cur_dataset = jsonlines.open(file_path)\n",
    "        for cur_data in cur_dataset:\n",
    "            cur_data[\"entity\"] = list(set(cur_data[\"entity\"]))\n",
    "            while cur_data['问题'][-1] == '；':\n",
    "                cur_data['问题'] = cur_data['问题'][:-1]   \n",
    "            all_data.append(cur_data)\n",
    "    random.shuffle(all_data)\n",
    "    train_data = all_data[0:int(len(all_data)*split_)]\n",
    "    test_data = all_data[int(len(all_data)*split_):]\n",
    "    id1 = 0\n",
    "    id2 = 0\n",
    "    for tr in train_data:\n",
    "        tr['id'] = id1\n",
    "        id1+=1\n",
    "        save = open(train_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(tr, ensure_ascii=False) + '\\n')\n",
    "        save.close()\n",
    "    for te in test_data:\n",
    "        te['id'] = id2\n",
    "        id2+=1\n",
    "        save = open(test_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(te, ensure_ascii=False) + '\\n')\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bea8a1-6689-43f9-8d87-879d8b9b5f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_relpath_prompt = \"你是一个中药复方知识图谱问答机器人，请生成一个或多个有效的关系路径，每个关系必须是[草药、效用、疾病、症候、症状、靶点、通路、成分、效应物]中的一个，该路径有助于回答以下问题：\\n\"\n",
    "SOP = \"<关系路径>{}</关系路径>\"\n",
    "SEP = \"<分隔>\"\n",
    "ent_reg_prompt = \"你是一个中药复方知识图谱问答机器人，请从输入的问题中提取有效的中药复方相关实体，该实体必须出现在输入中。\\n\"\n",
    "SOPREG = \"<实体>{}</实体>\"\n",
    "distur_prompt = \"你是一个中药复方知识图谱问答机器人，对于输入的问题、实体、关系路径，请根据问题的语义，将关系路径分配给合适的实体，使得在知识图谱中，这些实体通过关系路径，可以搜索到问题的答案。\\n\"\n",
    "SOPDIS = \"<{}>{}</{}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04a6255-14a1-4d81-be84-7d632e622100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rel_2_token(rela_list):\n",
    "    rel_list = []\n",
    "    for rela in rela_list:\n",
    "        rel_list.append(SOP.format(SEP.join(rela)))\n",
    "    return ';'.join(rel_list)\n",
    "\n",
    "def ent_2_token(enti_list):\n",
    "    ent_list = []\n",
    "    for ent in enti_list:\n",
    "        ent_list.append(SOPREG.format(ent))\n",
    "    return ';'.join(ent_list)\n",
    "\n",
    "def distur_2_token(dis_dict):\n",
    "    distur_list = []\n",
    "    for key, path in dis_dict.items():\n",
    "        rel_list_key = []\n",
    "        for rela in path:\n",
    "            rel_list_key.append(SOP.format(SEP.join(rela)))\n",
    "        distur_list.append(SOPDIS.format(key, ';'.join(rel_list_key), key))\n",
    "    return ';'.join(distur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61d1341-b256-4ca5-8ea9-e6bccd8295d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_fine_form_chatglm(train_data, is_shuffle, save_file):\n",
    "    output_list = []\n",
    "    cur_dataset = jsonlines.open(train_data)\n",
    "    for data in cur_dataset:\n",
    "        gen_rel = dict()\n",
    "        gen_rel['context'] = gen_relpath_prompt + data['问题']\n",
    "        rel_list = []\n",
    "        for rela in data['rel_path']:\n",
    "            rel_list.append(SOP.format(SEP.join(rela)))\n",
    "        gen_rel['target'] = ';'.join(rel_list)\n",
    "        ent_reg = dict()\n",
    "        ent_reg['context'] = ent_reg_prompt + data['问题']\n",
    "        ent_list = []\n",
    "        for ent in data['entity']:\n",
    "            ent_list.append(SOPREG.format(ent))\n",
    "        ent_reg['target'] = ';'.join(ent_list)\n",
    "        distur = dict()\n",
    "        distur['context'] = distur_prompt + '问题：' + data['问题'] + '\\n' + '实体：' + ';'.join(data['entity']) + '\\n' + '关系路径：' + ';'.join(rel_list)\n",
    "        distur_list = []\n",
    "        for key, path in data['distur'].items():\n",
    "            rel_list_key = []\n",
    "            for rela in path:\n",
    "                rel_list_key.append(SOP.format(SEP.join(rela)))\n",
    "            distur_list.append(SOPDIS.format(key, ';'.join(rel_list_key), key))\n",
    "        distur['target'] = ';'.join(distur_list)\n",
    "        output_list.append(gen_rel)\n",
    "        output_list.append(ent_reg)\n",
    "        output_list.append(distur)\n",
    "    if is_shuffle:\n",
    "        random.shuffle(output_list)\n",
    "    for data in output_list:\n",
    "        save = open(save_file, 'a',encoding='utf-8')\n",
    "        data_json = json.dumps(data,ensure_ascii=False)\n",
    "        save.write('{}\\n'.format(data_json))\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82bda9d4-80bd-429a-a2f8-aba41b1194ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_chatglm('./traindata.jsonl', True, './chatglm_fine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8eca38f-e7ce-4254-847f-413f078bf754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_2_rel(output):\n",
    "    rel_list = []\n",
    "    all_rel = ['草药','效用','疾病','症候','症状','靶点','通路','成分','效应物']\n",
    "    pattern = r\"<关系路径>(.*?)</关系路径>\"\n",
    "    for tokenstr in output:\n",
    "        matched_items = re.findall(pattern, tokenstr)\n",
    "        for items in matched_items:\n",
    "            items_list = items.split('<分隔>')\n",
    "            #print(items_list)\n",
    "            isvaild = True\n",
    "            for item in items_list:\n",
    "                if item not in all_rel:\n",
    "                    isvaild = False\n",
    "            if isvaild == True:\n",
    "                if items_list not in rel_list:\n",
    "                    rel_list.append(items_list)\n",
    "    return rel_list         \n",
    "\n",
    "def token_2_ent(output):\n",
    "    ent_list = set()\n",
    "    pattern = r\"<实体>(.*?)</实体>\"\n",
    "    matched_items = re.findall(pattern, output)\n",
    "    for en in matched_items:\n",
    "        ent_list.add(en)\n",
    "    return list(ent_list)\n",
    "\n",
    "def token_2_distur(output, ent_list):\n",
    "    distur = dict()\n",
    "    for entity in ent_list:\n",
    "        distur[entity] = []\n",
    "        pattern = r\"<{}>(.*?)</{}>\".format(entity,entity)\n",
    "        matched_items = re.findall(pattern, output)\n",
    "        rel_list = token_2_rel([output])\n",
    "        for rel in rel_list:\n",
    "            distur[entity].append(rel)\n",
    "    return distur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a43030-e152-40f9-8762-5bf8ffecdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_form_chatglm(test_data, save_file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b4c980c-84c3-4533-ac0e-ee70079002d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['靶点', '草药<分隔>效用']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 定义正则表达式，用于匹配关系路径标签内的内容\n",
    "pattern = r\"<{}>(.*?)</{}>\".format('关系路径','关系路径')\n",
    "\n",
    "# 给定的字符串，包含多个关系路径标签\n",
    "text = \"<关系路径>靶点</关系路径><关系路径>草药<分隔>效用</关系路径></\"\n",
    "\n",
    "# 使用re.findall方法查找所有匹配的内容\n",
    "matched_items = re.findall(pattern, text)\n",
    "\n",
    "# 输出匹配结果\n",
    "print(matched_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a58e6e-a912-46c4-8c2b-5f01515f661f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_glm4_form(system, user, ass):\n",
    "    gen_rel = dict()\n",
    "    gen_rel['messages'] = []\n",
    "    rel_sys = dict()\n",
    "    rel_sys['content'] = system\n",
    "    rel_sys['role'] = 'system'\n",
    "    rel_user = dict()\n",
    "    rel_user['content'] = user\n",
    "    rel_user['role'] = 'user'\n",
    "    rel_ass = dict()\n",
    "    rel_ass['content'] = ass\n",
    "    rel_ass['role'] = \"assistant\"\n",
    "    gen_rel['messages'].append(rel_sys)\n",
    "    gen_rel['messages'].append(rel_user)\n",
    "    gen_rel['messages'].append(rel_ass)\n",
    "    return gen_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f26890b-5122-4388-b92f-e6fb33434e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_fine_form_glm4(train_data, is_shuffle, save_file):\n",
    "    output_list = []\n",
    "    cur_dataset = jsonlines.open(train_data)\n",
    "    for data in cur_dataset:\n",
    "        relpath = rel_2_token(data['rel_path'])\n",
    "        gen_rel = get_glm4_form(gen_relpath_prompt, data['问题'], rel_2_token(data['rel_path']))\n",
    "        ent_reg = get_glm4_form(ent_reg_prompt, data['问题'], ent_2_token(data['entity']))\n",
    "        distur_user = '问题：' + data['问题'] + '\\n' + '实体：' + ';'.join(data['entity']) + '\\n' + '关系路径：' + relpath\n",
    "        distur = get_glm4_form(distur_prompt, distur_user, distur_2_token(data['distur']))\n",
    "        output_list.append(gen_rel)\n",
    "        output_list.append(ent_reg)\n",
    "        output_list.append(distur)\n",
    "    if is_shuffle:\n",
    "        random.shuffle(output_list)\n",
    "    for data in output_list:\n",
    "        save = open(save_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da5065d-43e8-4f67-b003-283c91d2651c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_fine_form_glm4_arr(train_data, is_shuffle, arr_list, save_file):\n",
    "    output_list = []\n",
    "    cur_dataset = jsonlines.open(train_data)\n",
    "    for data in cur_dataset:\n",
    "        relpath = rel_2_token(data['rel_path'])\n",
    "        gen_rel = get_glm4_form(gen_relpath_prompt, data['问题'], rel_2_token(data['rel_path']))\n",
    "        ent_reg = get_glm4_form(ent_reg_prompt, data['问题'], ent_2_token(data['entity']))\n",
    "        distur_user = '问题：' + data['问题'] + '\\n' + '实体：' + ';'.join(data['entity']) + '\\n' + '关系路径：' + relpath\n",
    "        distur = get_glm4_form(distur_prompt, distur_user, distur_2_token(data['distur']))\n",
    "        if arr_list[0] == 1: \n",
    "            output_list.append(gen_rel)\n",
    "        if arr_list[1] == 1:\n",
    "            output_list.append(ent_reg)\n",
    "        if arr_list[2] == 1:\n",
    "            output_list.append(distur)\n",
    "    if is_shuffle:\n",
    "        random.shuffle(output_list)\n",
    "    for data in output_list:\n",
    "        save = open(save_file, 'a',encoding='utf-8')\n",
    "        save.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "        save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6c16ac7-4941-4aa5-b82d-26b81a5fa57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4('./traindata.jsonl', True, './glm4/glm4_fine.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb9f585-f646-40b3-8698-3a1c99dab03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llama3_form(instruction, inputs, output):\n",
    "    gen_rel = dict()\n",
    "    gen_rel['instruction'] = instruction\n",
    "    gen_rel['input'] = inputs\n",
    "    gen_rel['output'] = output\n",
    "    return gen_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec5a369-061d-4b31-b355-1af51d2e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fine_form_llama3(train_data, is_shuffle, save_file):\n",
    "    output_list = []\n",
    "    cur_dataset = jsonlines.open(train_data)\n",
    "    for data in cur_dataset:\n",
    "        relpath = rel_2_token(data['rel_path'])\n",
    "        gen_rel = get_llama3_form(gen_relpath_prompt, data['问题'], rel_2_token(data['rel_path']))\n",
    "        ent_reg = get_llama3_form(ent_reg_prompt, data['问题'], ent_2_token(data['entity']))\n",
    "        distur_user = '问题：' + data['问题'] + '\\n' + '实体：' + ';'.join(data['entity']) + '\\n' + '关系路径：' + relpath\n",
    "        distur = get_llama3_form(distur_prompt, distur_user, distur_2_token(data['distur']))\n",
    "        output_list.append(gen_rel)\n",
    "        output_list.append(ent_reg)\n",
    "        output_list.append(distur)\n",
    "    if is_shuffle:\n",
    "        random.shuffle(output_list)\n",
    "    save = open(save_file, 'a',encoding='utf-8')\n",
    "    save.write(json.dumps(output_list, ensure_ascii=False))\n",
    "    save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78ba6ec9-4f44-4271-836f-d0a3f246b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_llama3('./traindata.jsonl', True, './llama3/llama3_fine.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262051e-463f-4547-9fec-3eda971c56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'<l(+)-精氨酸>(.*?)</l(+)-精氨酸>' '<l(+)-精氨酸><关系路径>草药</关系路径></l(+)-精氨酸>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2bd670c-7b61-42d8-9044-2a4ded2f7c99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m matched_items \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<l(+)-精氨酸>(.*?)</l(+)-精氨酸>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<l(+)-精氨酸><关系路径>草药</关系路径></l(+)-精氨酸>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:216\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[38;5;241m=\u001b[39m _compiler\u001b[38;5;241m.\u001b[39mcompile(pattern, flags)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse(p, flags)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:980\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    977\u001b[0m state\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m flags\n\u001b[0;32m    978\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m--> 980\u001b[0m p \u001b[38;5;241m=\u001b[39m _parse_sub(source, state, flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    981\u001b[0m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m fix_flags(\u001b[38;5;28mstr\u001b[39m, p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:455\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    453\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    456\u001b[0m                        \u001b[38;5;129;01mnot\u001b[39;00m nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items))\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:863\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    860\u001b[0m     group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    861\u001b[0m sub_verbose \u001b[38;5;241m=\u001b[39m ((verbose \u001b[38;5;129;01mor\u001b[39;00m (add_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    862\u001b[0m                \u001b[38;5;129;01mnot\u001b[39;00m (del_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[1;32m--> 863\u001b[0m p \u001b[38;5;241m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing ), unterminated subpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    866\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:455\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    453\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    456\u001b[0m                        \u001b[38;5;129;01mnot\u001b[39;00m nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items))\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\_parser.py:682\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    680\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item \u001b[38;5;129;01mor\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m AT:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnothing to repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    683\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _REPEATCODES:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    686\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n",
      "\u001b[1;31merror\u001b[0m: nothing to repeat at position 3"
     ]
    }
   ],
   "source": [
    "import re\n",
    "matched_items = re.findall(r'<l(+)-精氨酸>(.*?)</l(+)-精氨酸>', '<l(+)-精氨酸><关系路径>草药</关系路径></l(+)-精氨酸>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca5c7b5-e5e9-4e8e-b939-224779919294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec813a8-4073-4529-91e8-9376c2d6a813",
   "metadata": {},
   "source": [
    "## 修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7943806f-1604-423c-b4fb-35bb39c4a336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [7, 8]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "def find_all_occurrences(pattern, string):\n",
    "    matches = re.finditer(pattern, string)\n",
    "    start_index = [match.start() for match in matches]\n",
    "    answer = [[start,start+len(pattern)-1] for start in start_index]\n",
    "    return answer\n",
    " \n",
    "# 示例使用\n",
    "pattern = \"麝香\"\n",
    "string = \"麝香保心丸中的麝香\"\n",
    "indices = find_all_occurrences(pattern, string)\n",
    "print(indices)\n",
    "\n",
    "def search_decide(bigindex, smallindex):\n",
    "    isalone = False\n",
    "    for curindex in smallindex:\n",
    "        cur_bool = []\n",
    "        for curbigindex in bigindex:\n",
    "            if curindex[0] >= curbigindex[0] and curindex[1] <= curbigindex[1]:\n",
    "                cur_bool.append(False)\n",
    "            else: \n",
    "                cur_bool.append(True)\n",
    "        isalone = all(cur_bool)\n",
    "        if isalone == True:\n",
    "            return isalone\n",
    "        else:\n",
    "            continue\n",
    "    return isalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d76e71b6-8b56-44a0-9ee1-4ace378c5819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_decide([[0,8]], [[0, 1], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b700502-6d1f-4dec-9098-277c379b11f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac594f7-bffc-43db-84c6-2e626375d1a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 ['麝香', '麝香保心丸']\n",
      "31 ['麝香保心丸']\n",
      "60 ['麝香', '麝香保心丸']\n",
      "60 ['麝香保心丸']\n",
      "86 ['麝香', '麝香保心丸']\n",
      "86 ['麝香保心丸']\n",
      "125 ['麝香', '麝香保心丸']\n",
      "125 ['麝香保心丸']\n",
      "183 ['麝香', '麝香保心丸']\n",
      "183 ['麝香保心丸']\n",
      "215 ['arid5b', '麝香', '麝香保心丸']\n",
      "215 ['arid5b', '麝香保心丸']\n",
      "308 ['麝香', '麝香保心丸']\n",
      "308 ['麝香保心丸']\n",
      "325 ['BDPLT1', '麝香', '麝香保心丸']\n",
      "325 ['BDPLT1', '麝香保心丸']\n",
      "329 ['麝香', '麝香保心丸']\n",
      "329 ['麝香保心丸']\n",
      "356 ['麝香', '麝香保心丸']\n",
      "356 ['麝香保心丸']\n",
      "358 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "358 ['下瘀血汤', '麝香保心丸']\n",
      "365 ['麝香', '麝香保心丸']\n",
      "365 ['麝香保心丸']\n",
      "402 ['麝香', '麝香保心丸']\n",
      "402 ['麝香保心丸']\n",
      "414 ['麝香', '麝香保心丸']\n",
      "414 ['麝香保心丸']\n",
      "449 ['麝香', '麝香保心丸']\n",
      "449 ['麝香保心丸']\n",
      "501 ['麝香', '麝香保心丸', '心脏保护']\n",
      "501 ['麝香保心丸', '心脏保护']\n",
      "516 ['麝香', '麝香保心丸']\n",
      "516 ['麝香保心丸']\n",
      "542 ['抑制肝星状细胞增殖活力', '下瘀血汤', '麝香', '麝香保心丸']\n",
      "542 ['抑制肝星状细胞增殖活力', '下瘀血汤', '麝香保心丸']\n",
      "580 ['麝香', '麝香保心丸']\n",
      "580 ['麝香保心丸']\n",
      "599 ['麝香', '麝香保心丸']\n",
      "599 ['麝香保心丸']\n",
      "671 ['苍术苷a', '麝香', '麝香保心丸']\n",
      "671 ['苍术苷a', '麝香保心丸']\n",
      "698 ['麝香', '麝香保心丸']\n",
      "698 ['麝香保心丸']\n",
      "715 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "715 ['下瘀血汤', '麝香保心丸']\n",
      "810 ['麝香', '麝香保心丸']\n",
      "810 ['麝香保心丸']\n",
      "835 ['麝香', '麝香保心丸']\n",
      "835 ['麝香保心丸']\n",
      "847 ['麝香', '麝香保心丸']\n",
      "847 ['麝香保心丸']\n",
      "869 ['麝香', '麝香保心丸']\n",
      "869 ['麝香保心丸']\n",
      "884 ['麝香', '麝香保心丸']\n",
      "884 ['麝香保心丸']\n",
      "899 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "899 ['下瘀血汤', '麝香保心丸']\n",
      "900 ['麝香', '麝香保心丸']\n",
      "900 ['麝香保心丸']\n",
      "919 ['麝香', '麝香保心丸']\n",
      "919 ['麝香保心丸']\n",
      "1004 ['抗肝纤维化', '下瘀血汤', '麝香', '麝香保心丸']\n",
      "1004 ['抗肝纤维化', '下瘀血汤', '麝香保心丸']\n",
      "1012 ['麝香', '麝香保心丸']\n",
      "1012 ['麝香保心丸']\n",
      "1026 ['麝香', '麝香保心丸']\n",
      "1026 ['麝香保心丸']\n",
      "1036 ['麝香', '麝香保心丸']\n",
      "1036 ['麝香保心丸']\n",
      "1117 ['麝香', '麝香保心丸']\n",
      "1117 ['麝香保心丸']\n",
      "1155 ['麝香', '麝香保心丸']\n",
      "1155 ['麝香保心丸']\n",
      "1237 ['麝香', '麝香保心丸']\n",
      "1237 ['麝香保心丸']\n",
      "1239 ['麝香', '麝香保心丸']\n",
      "1239 ['麝香保心丸']\n",
      "1274 ['麝香', 'caskin1', '麝香保心丸']\n",
      "1274 ['caskin1', '麝香保心丸']\n",
      "1282 ['麝香', '麝香保心丸']\n",
      "1282 ['麝香保心丸']\n",
      "1290 ['麝香', '麝香保心丸']\n",
      "1290 ['麝香保心丸']\n",
      "1305 ['麝香', '麝香保心丸']\n",
      "1305 ['麝香保心丸']\n",
      "1315 ['麝香', '麝香保心丸']\n",
      "1315 ['麝香保心丸']\n",
      "1318 ['麝香', '麝香保心丸']\n",
      "1318 ['麝香保心丸']\n",
      "1320 ['麝香', '麝香保心丸']\n",
      "1320 ['麝香保心丸']\n",
      "1421 ['麝香', '麝香保心丸']\n",
      "1421 ['麝香保心丸']\n",
      "1479 ['下淤血汤', '麝香', '麝香保心丸']\n",
      "1479 ['下淤血汤', '麝香保心丸']\n",
      "1539 ['麝香', '麝香保心丸']\n",
      "1539 ['麝香保心丸']\n",
      "1599 ['麝香', '麝香保心丸']\n",
      "1599 ['麝香保心丸']\n",
      "1690 ['麝香', '麝香保心丸']\n",
      "1690 ['麝香保心丸']\n",
      "1735 ['麝香', '麝香保心丸']\n",
      "1735 ['麝香保心丸']\n",
      "1778 ['麝香', '麝香保心丸']\n",
      "1778 ['麝香保心丸']\n",
      "1780 ['四妙方', '麝香', '麝香保心丸']\n",
      "1780 ['四妙方', '麝香保心丸']\n",
      "1793 ['麝香', '麝香保心丸']\n",
      "1793 ['麝香保心丸']\n",
      "1823 ['麝香', '麝香保心丸']\n",
      "1823 ['麝香保心丸']\n",
      "1913 ['胸痛', '麝香', '麝香保心丸']\n",
      "1913 ['胸痛', '麝香保心丸']\n",
      "1934 ['麝香', '麝香保心丸']\n",
      "1934 ['麝香保心丸']\n",
      "1942 ['麝香', '麝香保心丸']\n",
      "1942 ['麝香保心丸']\n",
      "1994 ['麝香', 'akap12', '麝香保心丸']\n",
      "1994 ['akap12', '麝香保心丸']\n",
      "1997 ['四妙方', '麝香', '麝香保心丸']\n",
      "1997 ['四妙方', '麝香保心丸']\n",
      "2112 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "2112 ['下瘀血汤', '麝香保心丸']\n",
      "2147 ['l（+）-精氨酸', '麝香', '麝香保心丸']\n",
      "2147 ['l（+）-精氨酸', '麝香保心丸']\n",
      "2158 ['麝香', '麝香保心丸']\n",
      "2158 ['麝香保心丸']\n",
      "2187 ['麝香', '发热症状', 'unc13d', '麝香保心丸', '发热']\n",
      "2187 ['发热症状', 'unc13d', '麝香保心丸', '发热']\n",
      "2246 ['胸痛', '麝香', '麝香保心丸']\n",
      "2246 ['胸痛', '麝香保心丸']\n",
      "2264 ['麝香', '麝香保心丸']\n",
      "2264 ['麝香保心丸']\n",
      "2294 ['麝香', '麝香保心丸']\n",
      "2294 ['麝香保心丸']\n",
      "2323 ['麝香', '麝香保心丸']\n",
      "2323 ['麝香保心丸']\n",
      "2354 ['麝香', '麝香保心丸']\n",
      "2354 ['麝香保心丸']\n",
      "2362 ['麝香', '麝香保心丸']\n",
      "2362 ['麝香保心丸']\n",
      "2386 ['麝香', '麝香保心丸']\n",
      "2386 ['麝香保心丸']\n",
      "2429 ['麝香', '麝香保心丸']\n",
      "2429 ['麝香保心丸']\n",
      "2438 ['谷氨酰胺', '麝香', '麝香保心丸']\n",
      "2438 ['谷氨酰胺', '麝香保心丸']\n",
      "2445 ['麝香', '麝香保心丸']\n",
      "2445 ['麝香保心丸']\n",
      "2450 ['麝香', '麝香保心丸']\n",
      "2450 ['麝香保心丸']\n",
      "2451 ['冰片', '麝香', '麝香保心丸']\n",
      "2451 ['冰片', '麝香保心丸']\n",
      "2522 ['麝香', '麝香保心丸']\n",
      "2522 ['麝香保心丸']\n",
      "2534 ['abr', '麝香', '麝香保心丸']\n",
      "2534 ['abr', '麝香保心丸']\n",
      "2538 ['非酒精性脂肪肝病', '麝香', '麝香保心丸']\n",
      "2538 ['非酒精性脂肪肝病', '麝香保心丸']\n",
      "2557 ['麝香', '麝香保心丸']\n",
      "2557 ['麝香保心丸']\n",
      "2609 ['麝香', '麝香保心丸']\n",
      "2609 ['麝香保心丸']\n",
      "2613 ['麝香', '麝香保心丸']\n",
      "2613 ['麝香保心丸']\n",
      "2617 ['麝香', '麝香保心丸']\n",
      "2617 ['麝香保心丸']\n",
      "2658 ['jun', '麝香', '麝香保心丸']\n",
      "2658 ['jun', '麝香保心丸']\n",
      "2694 ['麝香', '麝香保心丸']\n",
      "2694 ['麝香保心丸']\n",
      "2735 ['麝香', '麝香保心丸']\n",
      "2735 ['麝香保心丸']\n",
      "2766 ['麝香', '麝香保心丸']\n",
      "2766 ['麝香保心丸']\n",
      "2791 ['苍术苷a', '麝香', '麝香保心丸']\n",
      "2791 ['苍术苷a', '麝香保心丸']\n",
      "2842 ['下淤血汤', 'abca3', '麝香', '麝香保心丸']\n",
      "2842 ['下淤血汤', 'abca3', '麝香保心丸']\n",
      "2911 ['麝香', '麝香保心丸']\n",
      "2911 ['麝香保心丸']\n",
      "2912 ['麝香', '麝香保心丸']\n",
      "2912 ['麝香保心丸']\n",
      "3013 ['麝香', '麝香保心丸']\n",
      "3013 ['麝香保心丸']\n",
      "3021 ['麝香', '麝香保心丸']\n",
      "3021 ['麝香保心丸']\n",
      "3034 ['黄柏', '麝香', '麝香保心丸']\n",
      "3034 ['黄柏', '麝香保心丸']\n",
      "3075 ['麝香', '麝香保心丸']\n",
      "3075 ['麝香保心丸']\n",
      "3126 ['麝香', '麝香保心丸']\n",
      "3126 ['麝香保心丸']\n",
      "3222 ['马卡因', '麝香', '麝香保心丸']\n",
      "3222 ['马卡因', '麝香保心丸']\n",
      "3242 ['麝香', '麝香保心丸']\n",
      "3242 ['麝香保心丸']\n",
      "3258 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "3258 ['下瘀血汤', '麝香保心丸']\n",
      "3306 ['麝香', '麝香保心丸']\n",
      "3306 ['麝香保心丸']\n",
      "3320 ['麝香', '麝香保心丸']\n",
      "3320 ['麝香保心丸']\n",
      "3407 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "3407 ['芪胶升白胶囊', '麝香保心丸']\n",
      "3474 ['ptpn18', '麝香', '麝香保心丸']\n",
      "3474 ['ptpn18', '麝香保心丸']\n",
      "3559 ['麝香', '麝香保心丸']\n",
      "3559 ['麝香保心丸']\n",
      "3561 ['麝香', '麝香保心丸']\n",
      "3561 ['麝香保心丸']\n",
      "3612 ['麝香', '麝香保心丸']\n",
      "3612 ['麝香保心丸']\n",
      "3653 ['麝香', '麝香保心丸']\n",
      "3653 ['麝香保心丸']\n",
      "3676 ['麝香', '麝香保心丸']\n",
      "3676 ['麝香保心丸']\n",
      "3683 ['麝香', '麝香保心丸']\n",
      "3683 ['麝香保心丸']\n",
      "3685 ['麝香', '麝香保心丸']\n",
      "3685 ['麝香保心丸']\n",
      "3691 ['麝香', '麝香保心丸']\n",
      "3691 ['麝香保心丸']\n",
      "3717 ['麝香', '麝香保心丸']\n",
      "3717 ['麝香保心丸']\n",
      "3728 ['四妙方', '麝香', '麝香保心丸']\n",
      "3728 ['四妙方', '麝香保心丸']\n",
      "3750 ['苍术苷a', '麝香', '麝香保心丸']\n",
      "3750 ['苍术苷a', '麝香保心丸']\n",
      "3824 ['BDPLT1', '麝香', '麝香保心丸']\n",
      "3824 ['BDPLT1', '麝香保心丸']\n",
      "3834 ['麝香', '麝香保心丸']\n",
      "3834 ['麝香保心丸']\n",
      "3837 ['麝香', 'L-脯氨酸', '麝香保心丸']\n",
      "3837 ['L-脯氨酸', '麝香保心丸']\n",
      "3853 ['麝香', '麝香保心丸']\n",
      "3853 ['麝香保心丸']\n",
      "3855 ['麝香', '麝香保心丸']\n",
      "3855 ['麝香保心丸']\n",
      "3862 ['苦参', '芪胶升白胶囊', '心肌梗死', '呼吸困难', '苦参酚a', '麝香保心丸']\n",
      "3862 ['芪胶升白胶囊', '心肌梗死', '呼吸困难', '苦参酚a', '麝香保心丸']\n",
      "3911 ['麝香', '麝香保心丸']\n",
      "3911 ['麝香保心丸']\n",
      "3922 ['麝香', '麝香保心丸']\n",
      "3922 ['麝香保心丸']\n",
      "3938 ['麝香', '麝香保心丸']\n",
      "3938 ['麝香保心丸']\n",
      "3943 ['麝香', '麝香保心丸']\n",
      "3943 ['麝香保心丸']\n",
      "3977 ['麝香', '麝香保心丸']\n",
      "3977 ['麝香保心丸']\n",
      "3989 ['麝香', '麝香保心丸']\n",
      "3989 ['麝香保心丸']\n",
      "4004 ['麝香', '麝香保心丸']\n",
      "4004 ['麝香保心丸']\n",
      "4108 ['麝香', '麝香保心丸']\n",
      "4108 ['麝香保心丸']\n",
      "4242 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "4242 ['下瘀血汤', '麝香保心丸']\n",
      "4271 ['四妙方', '麝香', '麝香保心丸']\n",
      "4271 ['四妙方', '麝香保心丸']\n",
      "4299 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "4299 ['芪胶升白胶囊', '麝香保心丸']\n",
      "4300 ['麝香', '麝香保心丸']\n",
      "4300 ['麝香保心丸']\n",
      "4341 ['麝香', '麝香保心丸']\n",
      "4341 ['麝香保心丸']\n",
      "4363 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "4363 ['芪胶升白胶囊', '麝香保心丸']\n",
      "4436 ['麝香', '麝香保心丸']\n",
      "4436 ['麝香保心丸']\n",
      "4450 ['麝香', '麝香保心丸']\n",
      "4450 ['麝香保心丸']\n",
      "4511 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "4511 ['下瘀血汤', '麝香保心丸']\n",
      "4516 ['麝香', '麝香保心丸']\n",
      "4516 ['麝香保心丸']\n",
      "4534 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "4534 ['芪胶升白胶囊', '麝香保心丸']\n",
      "4548 ['麝香', '麝香保心丸']\n",
      "4548 ['麝香保心丸']\n",
      "4570 ['麝香', '麝香保心丸']\n",
      "4570 ['麝香保心丸']\n",
      "4612 ['四妙方', '麝香', '麝香保心丸']\n",
      "4612 ['四妙方', '麝香保心丸']\n",
      "4752 ['麝香', '麝香保心丸']\n",
      "4752 ['麝香保心丸']\n",
      "4755 ['麝香', '麝香保心丸']\n",
      "4755 ['麝香保心丸']\n",
      "4767 ['麝香', '麝香保心丸']\n",
      "4767 ['麝香保心丸']\n",
      "4773 ['麝香', '麝香保心丸']\n",
      "4773 ['麝香保心丸']\n",
      "4796 ['abr', '麝香', '麝香保心丸']\n",
      "4796 ['abr', '麝香保心丸']\n",
      "4810 ['麝香', '麝香保心丸']\n",
      "4810 ['麝香保心丸']\n",
      "4824 ['谷氨酰胺', '麝香', '麝香保心丸']\n",
      "4824 ['谷氨酰胺', '麝香保心丸']\n",
      "4826 ['麝香', '麝香保心丸']\n",
      "4826 ['麝香保心丸']\n",
      "4859 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "4859 ['芪胶升白胶囊', '麝香保心丸']\n",
      "4978 ['下淤血汤', '麝香', '麝香保心丸']\n",
      "4978 ['下淤血汤', '麝香保心丸']\n",
      "4980 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "4980 ['下瘀血汤', '麝香保心丸']\n",
      "4983 ['麝香', '麝香保心丸']\n",
      "4983 ['麝香保心丸']\n",
      "5003 ['麝香', '麝香保心丸']\n",
      "5003 ['麝香保心丸']\n",
      "5070 ['麝香', '麝香保心丸']\n",
      "5070 ['麝香保心丸']\n",
      "5139 ['spta1', '麝香', '麝香保心丸']\n",
      "5139 ['spta1', '麝香保心丸']\n",
      "5215 ['麝香', '麝香保心丸']\n",
      "5215 ['麝香保心丸']\n",
      "5232 ['麝香', '麝香保心丸']\n",
      "5232 ['麝香保心丸']\n",
      "5281 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "5281 ['下瘀血汤', '麝香保心丸']\n",
      "5287 ['麝香', '麝香保心丸']\n",
      "5287 ['麝香保心丸']\n",
      "5291 ['蟾酥', '麝香', '麝香保心丸']\n",
      "5291 ['蟾酥', '麝香保心丸']\n",
      "5332 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "5332 ['下瘀血汤', '麝香保心丸']\n",
      "5360 ['麝香', '麝香保心丸']\n",
      "5360 ['麝香保心丸']\n",
      "5458 ['四妙方', 'hspa1a', '麝香', '麝香保心丸']\n",
      "5458 ['四妙方', 'hspa1a', '麝香保心丸']\n",
      "5466 ['麝香', '麝香保心丸']\n",
      "5466 ['麝香保心丸']\n",
      "5499 ['麝香', '麝香保心丸']\n",
      "5499 ['麝香保心丸']\n",
      "5515 ['麝香', '麝香保心丸']\n",
      "5515 ['麝香保心丸']\n",
      "5720 ['ptpn18', '麝香', '麝香保心丸']\n",
      "5720 ['ptpn18', '麝香保心丸']\n",
      "5722 ['麝香', '麝香保心丸']\n",
      "5722 ['麝香保心丸']\n",
      "5789 ['麝香', '麝香保心丸']\n",
      "5789 ['麝香保心丸']\n",
      "5806 ['麝香', '麝香保心丸']\n",
      "5806 ['麝香保心丸']\n",
      "5844 ['白细胞减少症', '麝香', '麝香保心丸']\n",
      "5844 ['白细胞减少症', '麝香保心丸']\n",
      "5889 ['麝香', '麝香保心丸']\n",
      "5889 ['麝香保心丸']\n",
      "5918 ['麝香', '麝香保心丸']\n",
      "5918 ['麝香保心丸']\n",
      "5959 ['麝香', '麝香保心丸']\n",
      "5959 ['麝香保心丸']\n",
      "5963 ['麝香', '麝香保心丸']\n",
      "5963 ['麝香保心丸']\n",
      "6041 ['四妙方', '麝香', '麝香保心丸']\n",
      "6041 ['四妙方', '麝香保心丸']\n",
      "6062 ['麝香', '麝香保心丸']\n",
      "6062 ['麝香保心丸']\n",
      "6090 ['蟾酥', '麝香', '麝香保心丸']\n",
      "6090 ['蟾酥', '麝香保心丸']\n",
      "6103 ['麝香', '麝香保心丸']\n",
      "6103 ['麝香保心丸']\n",
      "6127 ['麝香', '麝香保心丸']\n",
      "6127 ['麝香保心丸']\n",
      "6141 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "6141 ['芪胶升白胶囊', '麝香保心丸']\n",
      "6163 ['BDPLT1', '麝香', '麝香保心丸']\n",
      "6163 ['BDPLT1', '麝香保心丸']\n",
      "6190 ['黄柏', '麝香', '麝香保心丸']\n",
      "6190 ['黄柏', '麝香保心丸']\n",
      "6197 ['麝香', '麝香保心丸']\n",
      "6197 ['麝香保心丸']\n",
      "6250 ['麝香', '麝香保心丸']\n",
      "6250 ['麝香保心丸']\n",
      "6275 ['麝香', '麝香保心丸']\n",
      "6275 ['麝香保心丸']\n",
      "6330 ['心绞痛', '麝香', '麝香保心丸']\n",
      "6330 ['心绞痛', '麝香保心丸']\n",
      "6377 ['麝香', '麝香保心丸']\n",
      "6377 ['麝香保心丸']\n",
      "6401 ['麝香', '麝香保心丸']\n",
      "6401 ['麝香保心丸']\n",
      "6442 ['麝香', '麝香保心丸']\n",
      "6442 ['麝香保心丸']\n",
      "6535 ['麝香', '麝香保心丸']\n",
      "6535 ['麝香保心丸']\n",
      "6549 ['麝香', '麝香保心丸']\n",
      "6549 ['麝香保心丸']\n",
      "6583 ['麝香', '麝香保心丸']\n",
      "6583 ['麝香保心丸']\n",
      "6587 ['麝香', '麝香保心丸']\n",
      "6587 ['麝香保心丸']\n",
      "6622 ['l（+）-精氨酸', 'BDPLT1', '麝香', '麝香保心丸']\n",
      "6622 ['l（+）-精氨酸', 'BDPLT1', '麝香保心丸']\n",
      "6664 ['麝香', '麝香保心丸']\n",
      "6664 ['麝香保心丸']\n",
      "6674 ['BDPLT1', '麝香', '麝香保心丸']\n",
      "6674 ['BDPLT1', '麝香保心丸']\n",
      "6737 ['aldh1a1', '麝香', '麝香保心丸']\n",
      "6737 ['aldh1a1', '麝香保心丸']\n",
      "6758 ['麝香', '麝香保心丸']\n",
      "6758 ['麝香保心丸']\n",
      "6796 ['麝香', '麝香保心丸']\n",
      "6796 ['麝香保心丸']\n",
      "6825 ['麝香', '麝香保心丸']\n",
      "6825 ['麝香保心丸']\n",
      "6871 ['麝香', '麝香保心丸']\n",
      "6871 ['麝香保心丸']\n",
      "6905 ['麝香', '麝香保心丸']\n",
      "6905 ['麝香保心丸']\n",
      "6923 ['麝香', '麝香保心丸']\n",
      "6923 ['麝香保心丸']\n",
      "7000 ['麝香', '麝香保心丸']\n",
      "7000 ['麝香保心丸']\n",
      "7001 ['四妙方', '麝香', '麝香保心丸']\n",
      "7001 ['四妙方', '麝香保心丸']\n",
      "7038 ['麝香', '麝香保心丸']\n",
      "7038 ['麝香保心丸']\n",
      "7057 ['麝香', '麝香保心丸']\n",
      "7057 ['麝香保心丸']\n",
      "7060 ['麝香', '麝香保心丸']\n",
      "7060 ['麝香保心丸']\n",
      "7090 ['麝香', '麝香保心丸']\n",
      "7090 ['麝香保心丸']\n",
      "7156 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "7156 ['芪胶升白胶囊', '麝香保心丸']\n",
      "7207 ['麝香', '麝香保心丸']\n",
      "7207 ['麝香保心丸']\n",
      "7239 ['麝香', '麝香保心丸']\n",
      "7239 ['麝香保心丸']\n",
      "7240 ['L-脯氨酸', '麝香', '麝香保心丸']\n",
      "7240 ['L-脯氨酸', '麝香保心丸']\n",
      "7249 ['麝香', '麝香保心丸']\n",
      "7249 ['麝香保心丸']\n",
      "7323 ['麝香', '麝香保心丸']\n",
      "7323 ['麝香保心丸']\n",
      "7349 ['麝香', '麝香保心丸']\n",
      "7349 ['麝香保心丸']\n",
      "7352 ['麝香', '麝香保心丸']\n",
      "7352 ['麝香保心丸']\n",
      "7361 ['aldh1a1', '麝香', '麝香保心丸']\n",
      "7361 ['aldh1a1', '麝香保心丸']\n",
      "7375 ['L-脯氨酸', '麝香', '麝香保心丸']\n",
      "7375 ['L-脯氨酸', '麝香保心丸']\n",
      "7392 ['麝香', '麝香保心丸']\n",
      "7392 ['麝香保心丸']\n",
      "7403 ['麝香', '麝香保心丸']\n",
      "7403 ['麝香保心丸']\n",
      "7404 ['麝香', '麝香保心丸']\n",
      "7404 ['麝香保心丸']\n",
      "7417 ['麝香', '麝香保心丸']\n",
      "7417 ['麝香保心丸']\n",
      "7431 ['芪胶升白胶囊', '麝香', '麝香保心丸']\n",
      "7431 ['芪胶升白胶囊', '麝香保心丸']\n",
      "7459 ['麝香', '麝香保心丸']\n",
      "7459 ['麝香保心丸']\n",
      "7494 ['马卡因', '麝香', '麝香保心丸']\n",
      "7494 ['马卡因', '麝香保心丸']\n",
      "7506 ['麝香', '麝香保心丸']\n",
      "7506 ['麝香保心丸']\n",
      "7588 ['黄柏', '麝香', '麝香保心丸']\n",
      "7588 ['黄柏', '麝香保心丸']\n",
      "7594 ['麝香', '麝香保心丸']\n",
      "7594 ['麝香保心丸']\n",
      "7646 ['麝香', '麝香保心丸']\n",
      "7646 ['麝香保心丸']\n",
      "7669 ['麝香', '麝香保心丸']\n",
      "7669 ['麝香保心丸']\n",
      "7686 ['麝香', '麝香保心丸']\n",
      "7686 ['麝香保心丸']\n",
      "7710 ['麝香', '麝香保心丸']\n",
      "7710 ['麝香保心丸']\n",
      "7786 ['jun', '麝香', '麝香保心丸']\n",
      "7786 ['jun', '麝香保心丸']\n",
      "7792 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "7792 ['下瘀血汤', '麝香保心丸']\n",
      "7888 ['spta1', '麝香', '麝香保心丸']\n",
      "7888 ['spta1', '麝香保心丸']\n",
      "7889 ['麝香', '麝香保心丸']\n",
      "7889 ['麝香保心丸']\n",
      "7895 ['麝香', '麝香保心丸']\n",
      "7895 ['麝香保心丸']\n",
      "7909 ['麝香', '麝香保心丸']\n",
      "7909 ['麝香保心丸']\n",
      "7916 ['麝香', '麝香保心丸', '黄柏素']\n",
      "7916 ['麝香保心丸', '黄柏素']\n",
      "7925 ['BDPLT1', '麝香', '麝香保心丸']\n",
      "7925 ['BDPLT1', '麝香保心丸']\n",
      "7930 ['下瘀血汤', '麝香', '麝香保心丸']\n",
      "7930 ['下瘀血汤', '麝香保心丸']\n",
      "7935 ['非酒精性脂肪肝病', '麝香', '麝香保心丸']\n",
      "7935 ['非酒精性脂肪肝病', '麝香保心丸']\n",
      "7977 ['麝香', '麝香保心丸']\n",
      "7977 ['麝香保心丸']\n",
      "7982 ['麝香', '麝香保心丸']\n",
      "7982 ['麝香保心丸']\n"
     ]
    }
   ],
   "source": [
    "cur_dataset = jsonlines.open('./traindata.jsonl')\n",
    "new_data = []\n",
    "for data in cur_dataset:\n",
    "    repeatdic = dict()\n",
    "    cur_len = len(data['entity'])\n",
    "    new_ent = data['entity'].copy()\n",
    "    for i in range(cur_len):\n",
    "        for j in range(i+1, cur_len):\n",
    "            if data['entity'][i] in data['entity'][j]:\n",
    "                repeatdic[data['entity'][j]] = data['entity'][i]   \n",
    "    for bigent, smallent in repeatdic.items():\n",
    "        bigsearch = find_all_occurrences(bigent, data['问题'])\n",
    "        smallsearch = find_all_occurrences(smallent, data['问题'])\n",
    "        if not search_decide(bigsearch, smallsearch):\n",
    "            new_ent.remove(smallent)\n",
    "            #print(len(data['entity']))\n",
    "    if len(new_ent) != len(data['entity']):\n",
    "        print(data['id'], data['entity']) \n",
    "        data['entity'] = new_ent\n",
    "        print(data['id'], data['entity']) \n",
    "    new_data.append(data)\n",
    "for data in new_data:\n",
    "    save = open('./traindata_new.jsonl', 'a',encoding='utf-8')\n",
    "    save.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "    save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104ce6db-932e-4b1a-ba70-909b36df736c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b=a\n",
    "b[0] = 3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecdf7ee9-8eeb-45ee-9731-49dc0a306e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_check_shuffle_split(['./traindata_new.jsonl'], 0.2, './20%traindata.jsonl', './20%testdata.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7c09d2e-eb4d-46c7-988d-6f9a3fb02558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_check_shuffle_split(['./traindata_new.jsonl'], 0.4, './40%traindata.jsonl', './40%testdata.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60c8abbc-b5d4-4c06-9a26-05fa69952333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_check_shuffle_split(['./traindata_new.jsonl'], 0.6, './60%traindata.jsonl', './60%testdata.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "386cf322-68fa-442c-81d8-19c24775efdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_check_shuffle_split(['./traindata_new.jsonl'], 0.8, './80%traindata.jsonl', './80%testdata.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debb447f-61d4-4ff6-8d2e-21837d0566d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_check_shuffle_split(['./traindata_new.jsonl'], 0.05, './5%traindata.jsonl', './5%testdata.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e923a8d4-eafe-49d3-bcb8-f73c40f12f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_chatglm('./20%traindata.jsonl', True, './chatglm_20%fine.txt')\n",
    "gen_fine_form_chatglm('./40%traindata.jsonl', True, './chatglm_40%fine.txt')\n",
    "gen_fine_form_chatglm('./60%traindata.jsonl', True, './chatglm_60%fine.txt')\n",
    "gen_fine_form_chatglm('./80%traindata.jsonl', True, './chatglm_80%fine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4fd390-34d1-4843-9871-25dcaad2075b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_chatglm('./5%traindata.jsonl', True, './chatglm_5%fine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c7686ba-e6a8-4b53-a1cc-4369d352be6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4('./20%traindata.jsonl', True, './glm4/glm4_20%fine.jsonl')\n",
    "gen_fine_form_glm4('./40%traindata.jsonl', True, './glm4/glm4_40%fine.jsonl')\n",
    "gen_fine_form_glm4('./60%traindata.jsonl', True, './glm4/glm4_60%fine.jsonl')\n",
    "gen_fine_form_glm4('./80%traindata.jsonl', True, './glm4/glm4_80%fine.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c6d50f-d2a5-4151-a378-79222e9e46c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4('./5%traindata.jsonl', True, './glm4/glm4_5%fine.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56ccaf84-e9be-49ec-8760-04c72291de1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_llama3('./20%traindata.jsonl', True, './llama3/llama3_20%fine.json')\n",
    "gen_fine_form_llama3('./40%traindata.jsonl', True, './llama3/llama3_40%fine.json')\n",
    "gen_fine_form_llama3('./60%traindata.jsonl', True, './llama3/llama3_60%fine.json')\n",
    "gen_fine_form_llama3('./80%traindata.jsonl', True, './llama3/llama3_80%fine.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91183d4b-b4dc-42bd-b13e-4b49c1ce1bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_llama3('./5%traindata.jsonl', True, './llama3/llama3_5%fine.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbeeeaf8-8b27-427f-a873-ba58410b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [1,0,0], './glm4/glm4_fine_rel.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a800ce9-7551-48dd-be24-aab1099185c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [0,1,0], './glm4/glm4_fine_ent.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803fc5e9-09e7-425d-93e3-4f8d1d03bbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [0,0,1], './glm4/glm4_fine_dis.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f9ec76a-2917-4f29-be8a-3fee3139b0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [1,0,1], './glm4/glm4_fine_rel_dis.jsonl')\n",
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [0,1,1], './glm4/glm4_fine_ent_dis.jsonl')\n",
    "gen_fine_form_glm4_arr('./traindata_new.jsonl', True, [1,1,0], './glm4/glm4_fine_rel_ent.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5baef-8569-44ae-8d89-03b638ed035d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82e4fb86-dd14-49b0-8b71-c01578c48fb8",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93f699f-3bb1-4b30-8762-e2cb0fafa741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = jsonlines.open('./traindata_new.jsonl')\n",
    "answer = []\n",
    "for d in data:\n",
    "    if '麝香' in d['问题'] or '冠心病' in d['问题']:\n",
    "        continue\n",
    "    answer.append(d)\n",
    "for d in answer:\n",
    "    save = open('./traindata_rag.jsonl', 'a',encoding='utf-8')\n",
    "    save.write(json.dumps(d, ensure_ascii=False) + '\\n')\n",
    "    save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64819043-b644-405b-bb50-a0df6f803300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fine_form_glm4('./traindata_rag.jsonl', True, './glm4/glm4_ragfine.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8462bd0-2615-44b9-bf38-604b53a55a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = jsonlines.open('./traindata_new.jsonl')\n",
    "answer = []\n",
    "dis_set = set()\n",
    "for d in data:\n",
    "    if '麝香' in d['问题'] or '冠心病' in d['问题']:\n",
    "        if 'combine' not in d['class']:\n",
    "            if '麝香保心丸' in list(d['distur'].keys())[0]:\n",
    "                key = list(d['distur'].keys())[0]\n",
    "                value = '-'.join(d['distur'][key][0])\n",
    "                signal = key + '-' + value\n",
    "                if signal not in dis_set:\n",
    "                    dis_set.add(signal)\n",
    "                    d['rag'] = ''\n",
    "                    answer.append(d)\n",
    "for d in answer:\n",
    "    save = open('./testdata_rag.jsonl', 'a',encoding='utf-8')\n",
    "    save.write(json.dumps(d, ensure_ascii=False) + '\\n')\n",
    "    save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a5fc7-7876-4711-97a2-309afc905368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
