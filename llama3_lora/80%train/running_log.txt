[WARNING|parser.py:273] 2024-10-04 22:32:42,791 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.

[INFO|parser.py:325] 2024-10-04 22:32:42,792 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16

[INFO|tokenization_utils_base.py:2159] 2024-10-04 22:32:42,794 >> loading file tokenizer.json

[INFO|tokenization_utils_base.py:2159] 2024-10-04 22:32:42,794 >> loading file added_tokens.json

10/04/2024 22:32:42 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.

10/04/2024 22:32:42 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16

[INFO|tokenization_utils_base.py:2159] 2024-10-04 22:32:42,794 >> loading file special_tokens_map.json

[INFO|tokenization_utils_base.py:2159] 2024-10-04 22:32:42,794 >> loading file tokenizer_config.json

[WARNING|logging.py:313] 2024-10-04 22:32:43,073 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|template.py:270] 2024-10-04 22:32:43,074 >> Replace eos token: <|eot_id|>

[INFO|loader.py:50] 2024-10-04 22:32:43,075 >> Loading dataset llama3_80%fine.json...

10/04/2024 22:32:43 - WARNING - transformers.tokenization_utils_base - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

10/04/2024 22:32:43 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>

10/04/2024 22:32:45 - INFO - llamafactory.data.loader - Loading dataset llama3_80%fine.json...

[INFO|configuration_utils.py:731] 2024-10-04 22:32:48,614 >> loading configuration file /root/autodl-tmp/Llama3-8B-Chinese-Chat/config.json

[INFO|configuration_utils.py:800] 2024-10-04 22:32:48,615 >> Model config LlamaConfig {
  "_name_or_path": "/root/autodl-tmp/Llama3-8B-Chinese-Chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.4",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|modeling_utils.py:3553] 2024-10-04 22:32:48,636 >> loading weights file /root/autodl-tmp/Llama3-8B-Chinese-Chat/model.safetensors.index.json

[INFO|modeling_utils.py:1531] 2024-10-04 22:32:48,637 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.

[INFO|configuration_utils.py:1000] 2024-10-04 22:32:48,638 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009
}


[INFO|modeling_utils.py:4364] 2024-10-04 22:32:52,478 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


[INFO|modeling_utils.py:4372] 2024-10-04 22:32:52,478 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/Llama3-8B-Chinese-Chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

[INFO|configuration_utils.py:953] 2024-10-04 22:32:52,481 >> loading configuration file /root/autodl-tmp/Llama3-8B-Chinese-Chat/generation_config.json

[INFO|configuration_utils.py:1000] 2024-10-04 22:32:52,482 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "pad_token_id": 128009
}


[INFO|checkpointing.py:103] 2024-10-04 22:32:52,489 >> Gradient checkpointing enabled.

[INFO|attention.py:80] 2024-10-04 22:32:52,489 >> Using torch SDPA for faster training and inference.

[INFO|adapter.py:302] 2024-10-04 22:32:52,489 >> Upcasting trainable params to float32.

[INFO|adapter.py:158] 2024-10-04 22:32:52,489 >> Fine-tuning method: LoRA

[INFO|misc.py:51] 2024-10-04 22:32:52,490 >> Found linear modules: gate_proj,up_proj,q_proj,o_proj,k_proj,v_proj,down_proj

10/04/2024 22:32:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.

10/04/2024 22:32:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.

10/04/2024 22:32:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

10/04/2024 22:32:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

10/04/2024 22:32:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,q_proj,o_proj,v_proj,up_proj,down_proj

[INFO|loader.py:196] 2024-10-04 22:32:53,065 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605

[WARNING|other.py:349] 2024-10-04 22:32:53,069 >> Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

10/04/2024 22:32:53 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605

[INFO|trainer.py:642] 2024-10-04 22:32:53,080 >> Using auto half precision backend

[INFO|trainer.py:2128] 2024-10-04 22:32:53,624 >> ***** Running training *****

[INFO|trainer.py:2129] 2024-10-04 22:32:53,624 >>   Num examples = 19,200

[INFO|trainer.py:2130] 2024-10-04 22:32:53,624 >>   Num Epochs = 2

[INFO|trainer.py:2131] 2024-10-04 22:32:53,624 >>   Instantaneous batch size per device = 2

[INFO|trainer.py:2134] 2024-10-04 22:32:53,624 >>   Total train batch size (w. parallel, distributed & accumulation) = 32

[INFO|trainer.py:2135] 2024-10-04 22:32:53,624 >>   Gradient Accumulation steps = 8

[INFO|trainer.py:2136] 2024-10-04 22:32:53,624 >>   Total optimization steps = 1,200

[INFO|trainer.py:2137] 2024-10-04 22:32:53,629 >>   Number of trainable parameters = 20,971,520

[INFO|callbacks.py:310] 2024-10-04 22:33:04,071 >> {'loss': 1.8047, 'learning_rate': 4.9998e-05, 'epoch': 0.01, 'throughput': 2283.58}

[INFO|callbacks.py:310] 2024-10-04 22:33:13,526 >> {'loss': 0.5386, 'learning_rate': 4.9991e-05, 'epoch': 0.02, 'throughput': 2350.43}

[INFO|callbacks.py:310] 2024-10-04 22:33:22,931 >> {'loss': 0.1923, 'learning_rate': 4.9981e-05, 'epoch': 0.03, 'throughput': 2391.41}

[INFO|callbacks.py:310] 2024-10-04 22:33:32,392 >> {'loss': 0.0883, 'learning_rate': 4.9966e-05, 'epoch': 0.03, 'throughput': 2361.58}

[INFO|callbacks.py:310] 2024-10-04 22:33:41,868 >> {'loss': 0.0613, 'learning_rate': 4.9946e-05, 'epoch': 0.04, 'throughput': 2355.99}

[INFO|callbacks.py:310] 2024-10-04 22:33:51,449 >> {'loss': 0.0354, 'learning_rate': 4.9923e-05, 'epoch': 0.05, 'throughput': 2362.49}

[INFO|callbacks.py:310] 2024-10-04 22:34:00,994 >> {'loss': 0.0429, 'learning_rate': 4.9895e-05, 'epoch': 0.06, 'throughput': 2381.08}

[INFO|callbacks.py:310] 2024-10-04 22:34:10,629 >> {'loss': 0.0360, 'learning_rate': 4.9863e-05, 'epoch': 0.07, 'throughput': 2374.01}

[INFO|callbacks.py:310] 2024-10-04 22:34:20,146 >> {'loss': 0.0364, 'learning_rate': 4.9827e-05, 'epoch': 0.07, 'throughput': 2371.37}

[INFO|callbacks.py:310] 2024-10-04 22:34:29,729 >> {'loss': 0.0389, 'learning_rate': 4.9786e-05, 'epoch': 0.08, 'throughput': 2367.81}

[INFO|callbacks.py:310] 2024-10-04 22:34:39,272 >> {'loss': 0.0426, 'learning_rate': 4.9741e-05, 'epoch': 0.09, 'throughput': 2373.70}

[INFO|callbacks.py:310] 2024-10-04 22:34:48,920 >> {'loss': 0.0238, 'learning_rate': 4.9692e-05, 'epoch': 0.10, 'throughput': 2369.07}

[INFO|callbacks.py:310] 2024-10-04 22:34:58,444 >> {'loss': 0.0249, 'learning_rate': 4.9639e-05, 'epoch': 0.11, 'throughput': 2375.60}

[INFO|callbacks.py:310] 2024-10-04 22:35:07,921 >> {'loss': 0.0212, 'learning_rate': 4.9581e-05, 'epoch': 0.12, 'throughput': 2377.99}

[INFO|callbacks.py:310] 2024-10-04 22:35:17,412 >> {'loss': 0.0247, 'learning_rate': 4.9520e-05, 'epoch': 0.12, 'throughput': 2381.17}

[INFO|callbacks.py:310] 2024-10-04 22:35:26,961 >> {'loss': 0.0287, 'learning_rate': 4.9454e-05, 'epoch': 0.13, 'throughput': 2377.59}

[INFO|callbacks.py:310] 2024-10-04 22:35:36,390 >> {'loss': 0.0235, 'learning_rate': 4.9384e-05, 'epoch': 0.14, 'throughput': 2375.01}

[INFO|callbacks.py:310] 2024-10-04 22:35:45,949 >> {'loss': 0.0267, 'learning_rate': 4.9309e-05, 'epoch': 0.15, 'throughput': 2373.62}

[INFO|callbacks.py:310] 2024-10-04 22:35:55,439 >> {'loss': 0.0348, 'learning_rate': 4.9231e-05, 'epoch': 0.16, 'throughput': 2380.88}

[INFO|callbacks.py:310] 2024-10-04 22:36:05,025 >> {'loss': 0.0188, 'learning_rate': 4.9148e-05, 'epoch': 0.17, 'throughput': 2379.84}

[INFO|callbacks.py:310] 2024-10-04 22:36:14,679 >> {'loss': 0.0199, 'learning_rate': 4.9061e-05, 'epoch': 0.17, 'throughput': 2385.83}

[INFO|callbacks.py:310] 2024-10-04 22:36:24,195 >> {'loss': 0.0192, 'learning_rate': 4.8970e-05, 'epoch': 0.18, 'throughput': 2384.75}

[INFO|callbacks.py:310] 2024-10-04 22:36:33,776 >> {'loss': 0.0192, 'learning_rate': 4.8875e-05, 'epoch': 0.19, 'throughput': 2384.25}

[INFO|callbacks.py:310] 2024-10-04 22:36:43,335 >> {'loss': 0.0182, 'learning_rate': 4.8776e-05, 'epoch': 0.20, 'throughput': 2385.33}

[INFO|callbacks.py:310] 2024-10-04 22:36:52,932 >> {'loss': 0.0278, 'learning_rate': 4.8673e-05, 'epoch': 0.21, 'throughput': 2391.92}

[INFO|callbacks.py:310] 2024-10-04 22:37:02,484 >> {'loss': 0.0189, 'learning_rate': 4.8566e-05, 'epoch': 0.22, 'throughput': 2392.24}

[INFO|callbacks.py:310] 2024-10-04 22:37:12,003 >> {'loss': 0.0197, 'learning_rate': 4.8455e-05, 'epoch': 0.23, 'throughput': 2397.57}

[INFO|callbacks.py:310] 2024-10-04 22:37:21,413 >> {'loss': 0.0165, 'learning_rate': 4.8340e-05, 'epoch': 0.23, 'throughput': 2400.26}

[INFO|callbacks.py:310] 2024-10-04 22:37:30,882 >> {'loss': 0.0191, 'learning_rate': 4.8220e-05, 'epoch': 0.24, 'throughput': 2401.62}

[INFO|callbacks.py:310] 2024-10-04 22:37:40,275 >> {'loss': 0.0210, 'learning_rate': 4.8097e-05, 'epoch': 0.25, 'throughput': 2405.09}

[INFO|callbacks.py:310] 2024-10-04 22:37:49,775 >> {'loss': 0.0117, 'learning_rate': 4.7970e-05, 'epoch': 0.26, 'throughput': 2410.83}

[INFO|callbacks.py:310] 2024-10-04 22:37:59,214 >> {'loss': 0.0166, 'learning_rate': 4.7839e-05, 'epoch': 0.27, 'throughput': 2414.28}

[INFO|callbacks.py:310] 2024-10-04 22:38:08,660 >> {'loss': 0.0159, 'learning_rate': 4.7704e-05, 'epoch': 0.28, 'throughput': 2414.82}

[INFO|callbacks.py:310] 2024-10-04 22:38:18,115 >> {'loss': 0.0113, 'learning_rate': 4.7565e-05, 'epoch': 0.28, 'throughput': 2412.90}

[INFO|callbacks.py:310] 2024-10-04 22:38:27,600 >> {'loss': 0.0146, 'learning_rate': 4.7422e-05, 'epoch': 0.29, 'throughput': 2413.41}

[INFO|callbacks.py:310] 2024-10-04 22:38:37,114 >> {'loss': 0.0319, 'learning_rate': 4.7275e-05, 'epoch': 0.30, 'throughput': 2413.31}

[INFO|callbacks.py:310] 2024-10-04 22:38:46,653 >> {'loss': 0.0212, 'learning_rate': 4.7125e-05, 'epoch': 0.31, 'throughput': 2413.36}

[INFO|callbacks.py:310] 2024-10-04 22:38:56,227 >> {'loss': 0.0148, 'learning_rate': 4.6970e-05, 'epoch': 0.32, 'throughput': 2415.48}

[INFO|callbacks.py:310] 2024-10-04 22:39:05,709 >> {'loss': 0.0109, 'learning_rate': 4.6812e-05, 'epoch': 0.33, 'throughput': 2413.48}

[INFO|callbacks.py:310] 2024-10-04 22:39:15,160 >> {'loss': 0.0172, 'learning_rate': 4.6651e-05, 'epoch': 0.33, 'throughput': 2412.49}

[INFO|callbacks.py:310] 2024-10-04 22:39:24,513 >> {'loss': 0.0114, 'learning_rate': 4.6485e-05, 'epoch': 0.34, 'throughput': 2413.87}

[INFO|callbacks.py:310] 2024-10-04 22:39:33,865 >> {'loss': 0.0234, 'learning_rate': 4.6316e-05, 'epoch': 0.35, 'throughput': 2413.15}

[INFO|callbacks.py:310] 2024-10-04 22:39:43,248 >> {'loss': 0.0165, 'learning_rate': 4.6143e-05, 'epoch': 0.36, 'throughput': 2416.16}

[INFO|callbacks.py:310] 2024-10-04 22:39:52,689 >> {'loss': 0.0114, 'learning_rate': 4.5967e-05, 'epoch': 0.37, 'throughput': 2417.78}

[INFO|callbacks.py:310] 2024-10-04 22:40:02,045 >> {'loss': 0.0109, 'learning_rate': 4.5787e-05, 'epoch': 0.38, 'throughput': 2417.83}

[INFO|callbacks.py:310] 2024-10-04 22:40:11,468 >> {'loss': 0.0176, 'learning_rate': 4.5603e-05, 'epoch': 0.38, 'throughput': 2419.62}

[INFO|callbacks.py:310] 2024-10-04 22:40:20,822 >> {'loss': 0.0141, 'learning_rate': 4.5416e-05, 'epoch': 0.39, 'throughput': 2417.48}

[INFO|callbacks.py:310] 2024-10-04 22:40:30,300 >> {'loss': 0.0199, 'learning_rate': 4.5225e-05, 'epoch': 0.40, 'throughput': 2418.89}

[INFO|callbacks.py:310] 2024-10-04 22:40:39,763 >> {'loss': 0.0202, 'learning_rate': 4.5031e-05, 'epoch': 0.41, 'throughput': 2421.62}

[INFO|callbacks.py:310] 2024-10-04 22:40:49,434 >> {'loss': 0.0307, 'learning_rate': 4.4834e-05, 'epoch': 0.42, 'throughput': 2420.41}

[INFO|callbacks.py:310] 2024-10-04 22:40:58,876 >> {'loss': 0.0134, 'learning_rate': 4.4633e-05, 'epoch': 0.42, 'throughput': 2420.14}

[INFO|callbacks.py:310] 2024-10-04 22:41:08,321 >> {'loss': 0.0181, 'learning_rate': 4.4429e-05, 'epoch': 0.43, 'throughput': 2418.30}

[INFO|callbacks.py:310] 2024-10-04 22:41:17,786 >> {'loss': 0.0182, 'learning_rate': 4.4221e-05, 'epoch': 0.44, 'throughput': 2417.04}

[INFO|callbacks.py:310] 2024-10-04 22:41:27,253 >> {'loss': 0.0134, 'learning_rate': 4.4010e-05, 'epoch': 0.45, 'throughput': 2417.76}

[INFO|callbacks.py:310] 2024-10-04 22:41:36,709 >> {'loss': 0.0215, 'learning_rate': 4.3796e-05, 'epoch': 0.46, 'throughput': 2418.34}

[INFO|callbacks.py:310] 2024-10-04 22:41:46,211 >> {'loss': 0.0118, 'learning_rate': 4.3579e-05, 'epoch': 0.47, 'throughput': 2418.85}

[INFO|callbacks.py:310] 2024-10-04 22:41:55,680 >> {'loss': 0.0176, 'learning_rate': 4.3358e-05, 'epoch': 0.47, 'throughput': 2417.98}

[INFO|callbacks.py:310] 2024-10-04 22:42:05,178 >> {'loss': 0.0162, 'learning_rate': 4.3134e-05, 'epoch': 0.48, 'throughput': 2418.58}

[INFO|callbacks.py:310] 2024-10-04 22:42:14,684 >> {'loss': 0.0142, 'learning_rate': 4.2908e-05, 'epoch': 0.49, 'throughput': 2419.41}

[INFO|callbacks.py:310] 2024-10-04 22:42:24,196 >> {'loss': 0.0174, 'learning_rate': 4.2678e-05, 'epoch': 0.50, 'throughput': 2419.29}

[INFO|callbacks.py:310] 2024-10-04 22:42:33,733 >> {'loss': 0.0134, 'learning_rate': 4.2445e-05, 'epoch': 0.51, 'throughput': 2419.70}

[INFO|callbacks.py:310] 2024-10-04 22:42:43,253 >> {'loss': 0.0133, 'learning_rate': 4.2209e-05, 'epoch': 0.52, 'throughput': 2418.73}

[INFO|callbacks.py:310] 2024-10-04 22:42:52,853 >> {'loss': 0.0203, 'learning_rate': 4.1970e-05, 'epoch': 0.53, 'throughput': 2418.08}

[INFO|callbacks.py:310] 2024-10-04 22:43:02,374 >> {'loss': 0.0132, 'learning_rate': 4.1728e-05, 'epoch': 0.53, 'throughput': 2416.82}

[INFO|callbacks.py:310] 2024-10-04 22:43:11,964 >> {'loss': 0.0121, 'learning_rate': 4.1484e-05, 'epoch': 0.54, 'throughput': 2415.95}

[INFO|callbacks.py:310] 2024-10-04 22:43:21,495 >> {'loss': 0.0106, 'learning_rate': 4.1236e-05, 'epoch': 0.55, 'throughput': 2416.61}

[INFO|callbacks.py:310] 2024-10-04 22:43:31,144 >> {'loss': 0.0217, 'learning_rate': 4.0986e-05, 'epoch': 0.56, 'throughput': 2416.83}

[INFO|callbacks.py:310] 2024-10-04 22:43:40,444 >> {'loss': 0.0173, 'learning_rate': 4.0733e-05, 'epoch': 0.57, 'throughput': 2416.41}

[INFO|callbacks.py:310] 2024-10-04 22:43:49,766 >> {'loss': 0.0130, 'learning_rate': 4.0477e-05, 'epoch': 0.57, 'throughput': 2416.05}

[INFO|callbacks.py:310] 2024-10-04 22:43:59,082 >> {'loss': 0.0126, 'learning_rate': 4.0219e-05, 'epoch': 0.58, 'throughput': 2416.13}

[INFO|callbacks.py:310] 2024-10-04 22:44:08,482 >> {'loss': 0.0084, 'learning_rate': 3.9958e-05, 'epoch': 0.59, 'throughput': 2417.09}

[INFO|callbacks.py:310] 2024-10-04 22:44:17,847 >> {'loss': 0.0187, 'learning_rate': 3.9695e-05, 'epoch': 0.60, 'throughput': 2417.82}

[INFO|callbacks.py:310] 2024-10-04 22:44:27,204 >> {'loss': 0.0112, 'learning_rate': 3.9429e-05, 'epoch': 0.61, 'throughput': 2417.47}

[INFO|callbacks.py:310] 2024-10-04 22:44:36,534 >> {'loss': 0.0158, 'learning_rate': 3.9160e-05, 'epoch': 0.62, 'throughput': 2418.37}

[INFO|callbacks.py:310] 2024-10-04 22:44:46,016 >> {'loss': 0.0061, 'learning_rate': 3.8889e-05, 'epoch': 0.62, 'throughput': 2419.47}

[INFO|callbacks.py:310] 2024-10-04 22:44:55,469 >> {'loss': 0.0139, 'learning_rate': 3.8616e-05, 'epoch': 0.63, 'throughput': 2420.61}

[INFO|callbacks.py:310] 2024-10-04 22:45:05,030 >> {'loss': 0.0095, 'learning_rate': 3.8340e-05, 'epoch': 0.64, 'throughput': 2419.75}

[INFO|callbacks.py:310] 2024-10-04 22:45:14,505 >> {'loss': 0.0148, 'learning_rate': 3.8062e-05, 'epoch': 0.65, 'throughput': 2419.05}

[INFO|callbacks.py:310] 2024-10-04 22:45:23,958 >> {'loss': 0.0165, 'learning_rate': 3.7782e-05, 'epoch': 0.66, 'throughput': 2418.88}

[INFO|callbacks.py:310] 2024-10-04 22:45:33,419 >> {'loss': 0.0142, 'learning_rate': 3.7500e-05, 'epoch': 0.67, 'throughput': 2418.33}

[INFO|callbacks.py:310] 2024-10-04 22:45:43,097 >> {'loss': 0.0118, 'learning_rate': 3.7216e-05, 'epoch': 0.68, 'throughput': 2417.50}

[INFO|callbacks.py:310] 2024-10-04 22:45:52,609 >> {'loss': 0.0092, 'learning_rate': 3.6929e-05, 'epoch': 0.68, 'throughput': 2417.12}

[INFO|callbacks.py:310] 2024-10-04 22:46:02,060 >> {'loss': 0.0074, 'learning_rate': 3.6640e-05, 'epoch': 0.69, 'throughput': 2417.33}

[INFO|callbacks.py:310] 2024-10-04 22:46:11,597 >> {'loss': 0.0073, 'learning_rate': 3.6350e-05, 'epoch': 0.70, 'throughput': 2418.78}

[INFO|callbacks.py:310] 2024-10-04 22:46:21,069 >> {'loss': 0.0131, 'learning_rate': 3.6057e-05, 'epoch': 0.71, 'throughput': 2418.80}

[INFO|callbacks.py:310] 2024-10-04 22:46:30,562 >> {'loss': 0.0119, 'learning_rate': 3.5763e-05, 'epoch': 0.72, 'throughput': 2417.88}

[INFO|callbacks.py:310] 2024-10-04 22:46:40,002 >> {'loss': 0.0136, 'learning_rate': 3.5466e-05, 'epoch': 0.72, 'throughput': 2417.77}

[INFO|callbacks.py:310] 2024-10-04 22:46:49,495 >> {'loss': 0.0188, 'learning_rate': 3.5168e-05, 'epoch': 0.73, 'throughput': 2417.13}

[INFO|callbacks.py:310] 2024-10-04 22:46:59,019 >> {'loss': 0.0120, 'learning_rate': 3.4869e-05, 'epoch': 0.74, 'throughput': 2417.07}

[INFO|callbacks.py:310] 2024-10-04 22:47:08,575 >> {'loss': 0.0073, 'learning_rate': 3.4567e-05, 'epoch': 0.75, 'throughput': 2416.73}

[INFO|callbacks.py:310] 2024-10-04 22:47:18,125 >> {'loss': 0.0151, 'learning_rate': 3.4264e-05, 'epoch': 0.76, 'throughput': 2417.10}

[INFO|callbacks.py:310] 2024-10-04 22:47:27,630 >> {'loss': 0.0102, 'learning_rate': 3.3959e-05, 'epoch': 0.77, 'throughput': 2416.63}

[INFO|callbacks.py:310] 2024-10-04 22:47:37,116 >> {'loss': 0.0093, 'learning_rate': 3.3653e-05, 'epoch': 0.78, 'throughput': 2417.09}

[INFO|callbacks.py:310] 2024-10-04 22:47:46,603 >> {'loss': 0.0117, 'learning_rate': 3.3345e-05, 'epoch': 0.78, 'throughput': 2418.23}

[INFO|callbacks.py:310] 2024-10-04 22:47:56,046 >> {'loss': 0.0082, 'learning_rate': 3.3036e-05, 'epoch': 0.79, 'throughput': 2417.45}

[INFO|callbacks.py:310] 2024-10-04 22:48:05,517 >> {'loss': 0.0118, 'learning_rate': 3.2725e-05, 'epoch': 0.80, 'throughput': 2418.42}

[INFO|callbacks.py:310] 2024-10-04 22:48:14,889 >> {'loss': 0.0180, 'learning_rate': 3.2414e-05, 'epoch': 0.81, 'throughput': 2418.61}

[INFO|callbacks.py:310] 2024-10-04 22:48:24,297 >> {'loss': 0.0192, 'learning_rate': 3.2100e-05, 'epoch': 0.82, 'throughput': 2417.84}

[INFO|callbacks.py:310] 2024-10-04 22:48:33,733 >> {'loss': 0.0087, 'learning_rate': 3.1786e-05, 'epoch': 0.82, 'throughput': 2417.82}

[INFO|callbacks.py:310] 2024-10-04 22:48:43,158 >> {'loss': 0.0147, 'learning_rate': 3.1470e-05, 'epoch': 0.83, 'throughput': 2417.70}

[INFO|callbacks.py:310] 2024-10-04 22:48:52,576 >> {'loss': 0.0082, 'learning_rate': 3.1154e-05, 'epoch': 0.84, 'throughput': 2417.21}

[INFO|callbacks.py:310] 2024-10-04 22:49:02,039 >> {'loss': 0.0090, 'learning_rate': 3.0836e-05, 'epoch': 0.85, 'throughput': 2417.98}

[INFO|callbacks.py:310] 2024-10-04 22:49:11,518 >> {'loss': 0.0116, 'learning_rate': 3.0517e-05, 'epoch': 0.86, 'throughput': 2417.74}

[INFO|callbacks.py:310] 2024-10-04 22:49:21,105 >> {'loss': 0.0117, 'learning_rate': 3.0198e-05, 'epoch': 0.87, 'throughput': 2418.91}

[INFO|callbacks.py:310] 2024-10-04 22:49:30,687 >> {'loss': 0.0129, 'learning_rate': 2.9877e-05, 'epoch': 0.88, 'throughput': 2417.91}

[INFO|callbacks.py:310] 2024-10-04 22:49:40,093 >> {'loss': 0.0149, 'learning_rate': 2.9556e-05, 'epoch': 0.88, 'throughput': 2417.77}

[INFO|callbacks.py:310] 2024-10-04 22:49:49,537 >> {'loss': 0.0141, 'learning_rate': 2.9234e-05, 'epoch': 0.89, 'throughput': 2417.49}

[INFO|callbacks.py:310] 2024-10-04 22:49:58,942 >> {'loss': 0.0099, 'learning_rate': 2.8911e-05, 'epoch': 0.90, 'throughput': 2417.08}

[INFO|callbacks.py:310] 2024-10-04 22:50:08,351 >> {'loss': 0.0075, 'learning_rate': 2.8587e-05, 'epoch': 0.91, 'throughput': 2416.97}

[INFO|callbacks.py:310] 2024-10-04 22:50:17,729 >> {'loss': 0.0139, 'learning_rate': 2.8263e-05, 'epoch': 0.92, 'throughput': 2417.99}

[INFO|callbacks.py:310] 2024-10-04 22:50:27,209 >> {'loss': 0.0159, 'learning_rate': 2.7938e-05, 'epoch': 0.93, 'throughput': 2419.04}

[INFO|callbacks.py:310] 2024-10-04 22:50:36,636 >> {'loss': 0.0060, 'learning_rate': 2.7613e-05, 'epoch': 0.93, 'throughput': 2419.16}

[INFO|callbacks.py:310] 2024-10-04 22:50:46,084 >> {'loss': 0.0071, 'learning_rate': 2.7288e-05, 'epoch': 0.94, 'throughput': 2419.06}

[INFO|callbacks.py:310] 2024-10-04 22:50:55,455 >> {'loss': 0.0094, 'learning_rate': 2.6961e-05, 'epoch': 0.95, 'throughput': 2419.27}

[INFO|callbacks.py:310] 2024-10-04 22:51:04,849 >> {'loss': 0.0083, 'learning_rate': 2.6635e-05, 'epoch': 0.96, 'throughput': 2418.68}

[INFO|callbacks.py:310] 2024-10-04 22:51:14,335 >> {'loss': 0.0074, 'learning_rate': 2.6308e-05, 'epoch': 0.97, 'throughput': 2419.01}

[INFO|callbacks.py:310] 2024-10-04 22:51:23,805 >> {'loss': 0.0099, 'learning_rate': 2.5981e-05, 'epoch': 0.97, 'throughput': 2418.89}

[INFO|callbacks.py:310] 2024-10-04 22:51:33,262 >> {'loss': 0.0067, 'learning_rate': 2.5654e-05, 'epoch': 0.98, 'throughput': 2419.27}

[INFO|callbacks.py:310] 2024-10-04 22:51:42,823 >> {'loss': 0.0081, 'learning_rate': 2.5327e-05, 'epoch': 0.99, 'throughput': 2419.86}

[INFO|callbacks.py:310] 2024-10-04 22:51:52,338 >> {'loss': 0.0118, 'learning_rate': 2.5000e-05, 'epoch': 1.00, 'throughput': 2419.73}

[INFO|callbacks.py:310] 2024-10-04 22:52:01,855 >> {'loss': 0.0053, 'learning_rate': 2.4673e-05, 'epoch': 1.01, 'throughput': 2418.91}

[INFO|callbacks.py:310] 2024-10-04 22:52:11,368 >> {'loss': 0.0132, 'learning_rate': 2.4346e-05, 'epoch': 1.02, 'throughput': 2418.35}

[INFO|callbacks.py:310] 2024-10-04 22:52:20,780 >> {'loss': 0.0038, 'learning_rate': 2.4019e-05, 'epoch': 1.02, 'throughput': 2418.75}

[INFO|callbacks.py:310] 2024-10-04 22:52:30,177 >> {'loss': 0.0052, 'learning_rate': 2.3692e-05, 'epoch': 1.03, 'throughput': 2419.32}

[INFO|callbacks.py:310] 2024-10-04 22:52:39,605 >> {'loss': 0.0074, 'learning_rate': 2.3365e-05, 'epoch': 1.04, 'throughput': 2419.42}

[INFO|callbacks.py:310] 2024-10-04 22:52:49,055 >> {'loss': 0.0039, 'learning_rate': 2.3039e-05, 'epoch': 1.05, 'throughput': 2419.35}

[INFO|callbacks.py:310] 2024-10-04 22:52:58,500 >> {'loss': 0.0057, 'learning_rate': 2.2712e-05, 'epoch': 1.06, 'throughput': 2419.83}

[INFO|callbacks.py:310] 2024-10-04 22:53:07,951 >> {'loss': 0.0086, 'learning_rate': 2.2387e-05, 'epoch': 1.07, 'throughput': 2419.57}

[INFO|callbacks.py:310] 2024-10-04 22:53:17,380 >> {'loss': 0.0027, 'learning_rate': 2.2062e-05, 'epoch': 1.07, 'throughput': 2419.72}

[INFO|callbacks.py:310] 2024-10-04 22:53:26,865 >> {'loss': 0.0066, 'learning_rate': 2.1737e-05, 'epoch': 1.08, 'throughput': 2420.13}

[INFO|callbacks.py:310] 2024-10-04 22:53:36,309 >> {'loss': 0.0064, 'learning_rate': 2.1413e-05, 'epoch': 1.09, 'throughput': 2420.38}

[INFO|callbacks.py:310] 2024-10-04 22:53:45,745 >> {'loss': 0.0067, 'learning_rate': 2.1089e-05, 'epoch': 1.10, 'throughput': 2420.71}

[INFO|callbacks.py:310] 2024-10-04 22:53:55,194 >> {'loss': 0.0064, 'learning_rate': 2.0766e-05, 'epoch': 1.11, 'throughput': 2421.23}

[INFO|callbacks.py:310] 2024-10-04 22:54:04,721 >> {'loss': 0.0106, 'learning_rate': 2.0444e-05, 'epoch': 1.12, 'throughput': 2421.49}

[INFO|callbacks.py:310] 2024-10-04 22:54:14,140 >> {'loss': 0.0062, 'learning_rate': 2.0123e-05, 'epoch': 1.12, 'throughput': 2421.63}

[INFO|callbacks.py:310] 2024-10-04 22:54:23,592 >> {'loss': 0.0082, 'learning_rate': 1.9802e-05, 'epoch': 1.13, 'throughput': 2421.31}

[INFO|callbacks.py:310] 2024-10-04 22:54:32,891 >> {'loss': 0.0034, 'learning_rate': 1.9483e-05, 'epoch': 1.14, 'throughput': 2420.67}

[INFO|callbacks.py:310] 2024-10-04 22:54:42,233 >> {'loss': 0.0042, 'learning_rate': 1.9164e-05, 'epoch': 1.15, 'throughput': 2419.98}

[INFO|callbacks.py:310] 2024-10-04 22:54:51,571 >> {'loss': 0.0055, 'learning_rate': 1.8846e-05, 'epoch': 1.16, 'throughput': 2420.29}

[INFO|callbacks.py:310] 2024-10-04 22:55:00,936 >> {'loss': 0.0103, 'learning_rate': 1.8530e-05, 'epoch': 1.17, 'throughput': 2420.59}

[INFO|callbacks.py:310] 2024-10-04 22:55:10,288 >> {'loss': 0.0107, 'learning_rate': 1.8214e-05, 'epoch': 1.18, 'throughput': 2421.05}

[INFO|callbacks.py:310] 2024-10-04 22:55:19,601 >> {'loss': 0.0049, 'learning_rate': 1.7900e-05, 'epoch': 1.18, 'throughput': 2421.08}

[INFO|callbacks.py:310] 2024-10-04 22:55:29,013 >> {'loss': 0.0037, 'learning_rate': 1.7586e-05, 'epoch': 1.19, 'throughput': 2421.63}

[INFO|callbacks.py:310] 2024-10-04 22:55:38,536 >> {'loss': 0.0113, 'learning_rate': 1.7275e-05, 'epoch': 1.20, 'throughput': 2421.73}

[INFO|callbacks.py:310] 2024-10-04 22:55:48,001 >> {'loss': 0.0052, 'learning_rate': 1.6964e-05, 'epoch': 1.21, 'throughput': 2421.48}

[INFO|callbacks.py:310] 2024-10-04 22:55:57,614 >> {'loss': 0.0048, 'learning_rate': 1.6655e-05, 'epoch': 1.22, 'throughput': 2421.78}

[INFO|callbacks.py:310] 2024-10-04 22:56:07,202 >> {'loss': 0.0032, 'learning_rate': 1.6347e-05, 'epoch': 1.23, 'throughput': 2422.26}

[INFO|callbacks.py:310] 2024-10-04 22:56:16,739 >> {'loss': 0.0072, 'learning_rate': 1.6041e-05, 'epoch': 1.23, 'throughput': 2422.71}

[INFO|callbacks.py:310] 2024-10-04 22:56:26,323 >> {'loss': 0.0073, 'learning_rate': 1.5736e-05, 'epoch': 1.24, 'throughput': 2422.73}

[INFO|callbacks.py:310] 2024-10-04 22:56:35,861 >> {'loss': 0.0083, 'learning_rate': 1.5433e-05, 'epoch': 1.25, 'throughput': 2422.08}

[INFO|callbacks.py:310] 2024-10-04 22:56:45,252 >> {'loss': 0.0031, 'learning_rate': 1.5131e-05, 'epoch': 1.26, 'throughput': 2422.33}

[INFO|callbacks.py:310] 2024-10-04 22:56:54,672 >> {'loss': 0.0065, 'learning_rate': 1.4832e-05, 'epoch': 1.27, 'throughput': 2422.68}

[INFO|callbacks.py:310] 2024-10-04 22:57:04,149 >> {'loss': 0.0050, 'learning_rate': 1.4534e-05, 'epoch': 1.27, 'throughput': 2422.64}

[INFO|callbacks.py:310] 2024-10-04 22:57:13,497 >> {'loss': 0.0030, 'learning_rate': 1.4237e-05, 'epoch': 1.28, 'throughput': 2423.12}

[INFO|callbacks.py:310] 2024-10-04 22:57:22,951 >> {'loss': 0.0070, 'learning_rate': 1.3943e-05, 'epoch': 1.29, 'throughput': 2423.76}

[INFO|callbacks.py:310] 2024-10-04 22:57:32,248 >> {'loss': 0.0060, 'learning_rate': 1.3650e-05, 'epoch': 1.30, 'throughput': 2423.53}

[INFO|callbacks.py:310] 2024-10-04 22:57:41,775 >> {'loss': 0.0033, 'learning_rate': 1.3360e-05, 'epoch': 1.31, 'throughput': 2424.28}

[INFO|callbacks.py:310] 2024-10-04 22:57:51,247 >> {'loss': 0.0041, 'learning_rate': 1.3071e-05, 'epoch': 1.32, 'throughput': 2424.35}

[INFO|callbacks.py:310] 2024-10-04 22:58:00,739 >> {'loss': 0.0051, 'learning_rate': 1.2784e-05, 'epoch': 1.32, 'throughput': 2424.74}

[INFO|callbacks.py:310] 2024-10-04 22:58:10,179 >> {'loss': 0.0029, 'learning_rate': 1.2500e-05, 'epoch': 1.33, 'throughput': 2425.09}

[INFO|callbacks.py:310] 2024-10-04 22:58:19,638 >> {'loss': 0.0023, 'learning_rate': 1.2218e-05, 'epoch': 1.34, 'throughput': 2424.87}

[INFO|callbacks.py:310] 2024-10-04 22:58:29,028 >> {'loss': 0.0038, 'learning_rate': 1.1938e-05, 'epoch': 1.35, 'throughput': 2424.32}

[INFO|callbacks.py:310] 2024-10-04 22:58:38,452 >> {'loss': 0.0085, 'learning_rate': 1.1660e-05, 'epoch': 1.36, 'throughput': 2424.11}

[INFO|callbacks.py:310] 2024-10-04 22:58:47,966 >> {'loss': 0.0040, 'learning_rate': 1.1384e-05, 'epoch': 1.37, 'throughput': 2424.09}

[INFO|callbacks.py:310] 2024-10-04 22:58:57,501 >> {'loss': 0.0036, 'learning_rate': 1.1111e-05, 'epoch': 1.38, 'throughput': 2424.43}

[INFO|callbacks.py:310] 2024-10-04 22:59:07,079 >> {'loss': 0.0092, 'learning_rate': 1.0840e-05, 'epoch': 1.38, 'throughput': 2424.28}

[INFO|callbacks.py:310] 2024-10-04 22:59:16,602 >> {'loss': 0.0089, 'learning_rate': 1.0571e-05, 'epoch': 1.39, 'throughput': 2423.92}

[INFO|callbacks.py:310] 2024-10-04 22:59:26,128 >> {'loss': 0.0054, 'learning_rate': 1.0305e-05, 'epoch': 1.40, 'throughput': 2423.55}

[INFO|callbacks.py:310] 2024-10-04 22:59:35,685 >> {'loss': 0.0064, 'learning_rate': 1.0042e-05, 'epoch': 1.41, 'throughput': 2423.10}

[INFO|callbacks.py:310] 2024-10-04 22:59:45,297 >> {'loss': 0.0038, 'learning_rate': 9.7810e-06, 'epoch': 1.42, 'throughput': 2423.26}

[INFO|callbacks.py:310] 2024-10-04 22:59:54,962 >> {'loss': 0.0041, 'learning_rate': 9.5227e-06, 'epoch': 1.43, 'throughput': 2423.57}

[INFO|callbacks.py:310] 2024-10-04 23:00:04,583 >> {'loss': 0.0041, 'learning_rate': 9.2670e-06, 'epoch': 1.43, 'throughput': 2423.21}

[INFO|callbacks.py:310] 2024-10-04 23:00:14,163 >> {'loss': 0.0042, 'learning_rate': 9.0140e-06, 'epoch': 1.44, 'throughput': 2422.79}

[INFO|callbacks.py:310] 2024-10-04 23:00:23,796 >> {'loss': 0.0038, 'learning_rate': 8.7638e-06, 'epoch': 1.45, 'throughput': 2422.63}

[INFO|callbacks.py:310] 2024-10-04 23:00:33,432 >> {'loss': 0.0060, 'learning_rate': 8.5164e-06, 'epoch': 1.46, 'throughput': 2423.05}

[INFO|callbacks.py:310] 2024-10-04 23:00:43,040 >> {'loss': 0.0039, 'learning_rate': 8.2717e-06, 'epoch': 1.47, 'throughput': 2422.51}

[INFO|callbacks.py:310] 2024-10-04 23:00:52,741 >> {'loss': 0.0026, 'learning_rate': 8.0300e-06, 'epoch': 1.48, 'throughput': 2422.57}

[INFO|callbacks.py:310] 2024-10-04 23:01:02,337 >> {'loss': 0.0056, 'learning_rate': 7.7911e-06, 'epoch': 1.48, 'throughput': 2422.79}

[INFO|callbacks.py:310] 2024-10-04 23:01:11,851 >> {'loss': 0.0108, 'learning_rate': 7.5552e-06, 'epoch': 1.49, 'throughput': 2422.07}

[INFO|callbacks.py:310] 2024-10-04 23:01:21,416 >> {'loss': 0.0058, 'learning_rate': 7.3223e-06, 'epoch': 1.50, 'throughput': 2421.76}

[INFO|callbacks.py:310] 2024-10-04 23:01:30,950 >> {'loss': 0.0032, 'learning_rate': 7.0925e-06, 'epoch': 1.51, 'throughput': 2421.48}

[INFO|callbacks.py:310] 2024-10-04 23:01:40,567 >> {'loss': 0.0032, 'learning_rate': 6.8656e-06, 'epoch': 1.52, 'throughput': 2421.18}

[INFO|callbacks.py:310] 2024-10-04 23:01:50,096 >> {'loss': 0.0060, 'learning_rate': 6.6419e-06, 'epoch': 1.52, 'throughput': 2420.54}

[INFO|callbacks.py:310] 2024-10-04 23:01:59,614 >> {'loss': 0.0044, 'learning_rate': 6.4214e-06, 'epoch': 1.53, 'throughput': 2421.06}

[INFO|callbacks.py:310] 2024-10-04 23:02:09,234 >> {'loss': 0.0042, 'learning_rate': 6.2040e-06, 'epoch': 1.54, 'throughput': 2420.59}

[INFO|callbacks.py:310] 2024-10-04 23:02:18,791 >> {'loss': 0.0037, 'learning_rate': 5.9899e-06, 'epoch': 1.55, 'throughput': 2419.85}

[INFO|callbacks.py:310] 2024-10-04 23:02:28,419 >> {'loss': 0.0087, 'learning_rate': 5.7790e-06, 'epoch': 1.56, 'throughput': 2419.47}

[INFO|callbacks.py:310] 2024-10-04 23:02:38,038 >> {'loss': 0.0073, 'learning_rate': 5.5714e-06, 'epoch': 1.57, 'throughput': 2419.40}

[INFO|callbacks.py:310] 2024-10-04 23:02:47,760 >> {'loss': 0.0032, 'learning_rate': 5.3671e-06, 'epoch': 1.57, 'throughput': 2419.50}

[INFO|callbacks.py:310] 2024-10-04 23:02:57,360 >> {'loss': 0.0014, 'learning_rate': 5.1662e-06, 'epoch': 1.58, 'throughput': 2419.32}

[INFO|callbacks.py:310] 2024-10-04 23:03:06,992 >> {'loss': 0.0109, 'learning_rate': 4.9687e-06, 'epoch': 1.59, 'throughput': 2419.51}

[INFO|callbacks.py:310] 2024-10-04 23:03:16,468 >> {'loss': 0.0050, 'learning_rate': 4.7746e-06, 'epoch': 1.60, 'throughput': 2419.66}

[INFO|callbacks.py:310] 2024-10-04 23:03:25,990 >> {'loss': 0.0039, 'learning_rate': 4.5840e-06, 'epoch': 1.61, 'throughput': 2419.21}

[INFO|callbacks.py:310] 2024-10-04 23:03:35,482 >> {'loss': 0.0048, 'learning_rate': 4.3968e-06, 'epoch': 1.62, 'throughput': 2418.82}

[INFO|callbacks.py:310] 2024-10-04 23:03:45,104 >> {'loss': 0.0064, 'learning_rate': 4.2133e-06, 'epoch': 1.62, 'throughput': 2419.15}

[INFO|callbacks.py:310] 2024-10-04 23:03:54,692 >> {'loss': 0.0030, 'learning_rate': 4.0332e-06, 'epoch': 1.63, 'throughput': 2419.59}

[INFO|callbacks.py:310] 2024-10-04 23:04:04,275 >> {'loss': 0.0027, 'learning_rate': 3.8568e-06, 'epoch': 1.64, 'throughput': 2419.28}

[INFO|callbacks.py:310] 2024-10-04 23:04:13,914 >> {'loss': 0.0061, 'learning_rate': 3.6840e-06, 'epoch': 1.65, 'throughput': 2418.47}

[INFO|callbacks.py:310] 2024-10-04 23:04:23,586 >> {'loss': 0.0021, 'learning_rate': 3.5148e-06, 'epoch': 1.66, 'throughput': 2417.62}

[INFO|callbacks.py:310] 2024-10-04 23:04:33,234 >> {'loss': 0.0042, 'learning_rate': 3.3494e-06, 'epoch': 1.67, 'throughput': 2417.26}

[INFO|trainer.py:3478] 2024-10-04 23:04:33,235 >> Saving model checkpoint to saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1000

[INFO|tokenization_utils_base.py:2574] 2024-10-04 23:04:33,380 >> tokenizer config file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1000/tokenizer_config.json

[INFO|tokenization_utils_base.py:2583] 2024-10-04 23:04:33,381 >> Special tokens file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1000/special_tokens_map.json

[INFO|callbacks.py:310] 2024-10-04 23:04:43,357 >> {'loss': 0.0050, 'learning_rate': 3.1876e-06, 'epoch': 1.68, 'throughput': 2416.58}

[INFO|callbacks.py:310] 2024-10-04 23:04:53,022 >> {'loss': 0.0033, 'learning_rate': 3.0296e-06, 'epoch': 1.68, 'throughput': 2416.63}

[INFO|callbacks.py:310] 2024-10-04 23:05:02,601 >> {'loss': 0.0035, 'learning_rate': 2.8753e-06, 'epoch': 1.69, 'throughput': 2416.68}

[INFO|callbacks.py:310] 2024-10-04 23:05:12,301 >> {'loss': 0.0030, 'learning_rate': 2.7248e-06, 'epoch': 1.70, 'throughput': 2416.55}

[INFO|callbacks.py:310] 2024-10-04 23:05:21,753 >> {'loss': 0.0060, 'learning_rate': 2.5782e-06, 'epoch': 1.71, 'throughput': 2417.21}

[INFO|callbacks.py:310] 2024-10-04 23:05:31,201 >> {'loss': 0.0055, 'learning_rate': 2.4354e-06, 'epoch': 1.72, 'throughput': 2417.51}

[INFO|callbacks.py:310] 2024-10-04 23:05:40,614 >> {'loss': 0.0096, 'learning_rate': 2.2964e-06, 'epoch': 1.73, 'throughput': 2417.46}

[INFO|callbacks.py:310] 2024-10-04 23:05:50,062 >> {'loss': 0.0086, 'learning_rate': 2.1614e-06, 'epoch': 1.73, 'throughput': 2417.49}

[INFO|callbacks.py:310] 2024-10-04 23:05:59,438 >> {'loss': 0.0059, 'learning_rate': 2.0302e-06, 'epoch': 1.74, 'throughput': 2416.96}

[INFO|callbacks.py:310] 2024-10-04 23:06:08,894 >> {'loss': 0.0033, 'learning_rate': 1.9030e-06, 'epoch': 1.75, 'throughput': 2417.21}

[INFO|callbacks.py:310] 2024-10-04 23:06:18,320 >> {'loss': 0.0028, 'learning_rate': 1.7798e-06, 'epoch': 1.76, 'throughput': 2417.36}

[INFO|callbacks.py:310] 2024-10-04 23:06:27,919 >> {'loss': 0.0036, 'learning_rate': 1.6605e-06, 'epoch': 1.77, 'throughput': 2417.22}

[INFO|callbacks.py:310] 2024-10-04 23:06:37,493 >> {'loss': 0.0025, 'learning_rate': 1.5452e-06, 'epoch': 1.77, 'throughput': 2417.17}

[INFO|callbacks.py:310] 2024-10-04 23:06:47,053 >> {'loss': 0.0113, 'learning_rate': 1.4340e-06, 'epoch': 1.78, 'throughput': 2417.12}

[INFO|callbacks.py:310] 2024-10-04 23:06:56,621 >> {'loss': 0.0046, 'learning_rate': 1.3267e-06, 'epoch': 1.79, 'throughput': 2416.54}

[INFO|callbacks.py:310] 2024-10-04 23:07:06,189 >> {'loss': 0.0039, 'learning_rate': 1.2236e-06, 'epoch': 1.80, 'throughput': 2416.85}

[INFO|callbacks.py:310] 2024-10-04 23:07:15,829 >> {'loss': 0.0042, 'learning_rate': 1.1245e-06, 'epoch': 1.81, 'throughput': 2416.67}

[INFO|callbacks.py:310] 2024-10-04 23:07:25,568 >> {'loss': 0.0037, 'learning_rate': 1.0295e-06, 'epoch': 1.82, 'throughput': 2416.55}

[INFO|callbacks.py:310] 2024-10-04 23:07:35,104 >> {'loss': 0.0049, 'learning_rate': 9.3862e-07, 'epoch': 1.82, 'throughput': 2416.68}

[INFO|callbacks.py:310] 2024-10-04 23:07:44,496 >> {'loss': 0.0033, 'learning_rate': 8.5185e-07, 'epoch': 1.83, 'throughput': 2416.89}

[INFO|callbacks.py:310] 2024-10-04 23:07:53,950 >> {'loss': 0.0045, 'learning_rate': 7.6923e-07, 'epoch': 1.84, 'throughput': 2416.80}

[INFO|callbacks.py:310] 2024-10-04 23:08:03,376 >> {'loss': 0.0090, 'learning_rate': 6.9075e-07, 'epoch': 1.85, 'throughput': 2416.67}

[INFO|callbacks.py:310] 2024-10-04 23:08:12,826 >> {'loss': 0.0074, 'learning_rate': 6.1644e-07, 'epoch': 1.86, 'throughput': 2416.82}

[INFO|callbacks.py:310] 2024-10-04 23:08:22,211 >> {'loss': 0.0026, 'learning_rate': 5.4631e-07, 'epoch': 1.87, 'throughput': 2416.73}

[INFO|callbacks.py:310] 2024-10-04 23:08:31,725 >> {'loss': 0.0023, 'learning_rate': 4.8037e-07, 'epoch': 1.88, 'throughput': 2416.70}

[INFO|callbacks.py:310] 2024-10-04 23:08:41,307 >> {'loss': 0.0035, 'learning_rate': 4.1863e-07, 'epoch': 1.88, 'throughput': 2416.72}

[INFO|callbacks.py:310] 2024-10-04 23:08:50,862 >> {'loss': 0.0042, 'learning_rate': 3.6110e-07, 'epoch': 1.89, 'throughput': 2416.11}

[INFO|callbacks.py:310] 2024-10-04 23:09:00,414 >> {'loss': 0.0062, 'learning_rate': 3.0779e-07, 'epoch': 1.90, 'throughput': 2415.88}

[INFO|callbacks.py:310] 2024-10-04 23:09:10,014 >> {'loss': 0.0034, 'learning_rate': 2.5872e-07, 'epoch': 1.91, 'throughput': 2415.87}

[INFO|callbacks.py:310] 2024-10-04 23:09:19,548 >> {'loss': 0.0048, 'learning_rate': 2.1388e-07, 'epoch': 1.92, 'throughput': 2415.65}

[INFO|callbacks.py:310] 2024-10-04 23:09:29,247 >> {'loss': 0.0037, 'learning_rate': 1.7329e-07, 'epoch': 1.93, 'throughput': 2414.97}

[INFO|callbacks.py:310] 2024-10-04 23:09:38,709 >> {'loss': 0.0032, 'learning_rate': 1.3695e-07, 'epoch': 1.93, 'throughput': 2415.08}

[INFO|callbacks.py:310] 2024-10-04 23:09:48,227 >> {'loss': 0.0025, 'learning_rate': 1.0488e-07, 'epoch': 1.94, 'throughput': 2414.84}

[INFO|callbacks.py:310] 2024-10-04 23:09:57,718 >> {'loss': 0.0013, 'learning_rate': 7.7067e-08, 'epoch': 1.95, 'throughput': 2414.81}

[INFO|callbacks.py:310] 2024-10-04 23:10:07,171 >> {'loss': 0.0014, 'learning_rate': 5.3527e-08, 'epoch': 1.96, 'throughput': 2414.80}

[INFO|callbacks.py:310] 2024-10-04 23:10:16,666 >> {'loss': 0.0054, 'learning_rate': 3.4262e-08, 'epoch': 1.97, 'throughput': 2415.23}

[INFO|callbacks.py:310] 2024-10-04 23:10:26,110 >> {'loss': 0.0034, 'learning_rate': 1.9274e-08, 'epoch': 1.98, 'throughput': 2415.34}

[INFO|callbacks.py:310] 2024-10-04 23:10:35,569 >> {'loss': 0.0060, 'learning_rate': 8.5669e-09, 'epoch': 1.98, 'throughput': 2415.61}

[INFO|callbacks.py:310] 2024-10-04 23:10:45,005 >> {'loss': 0.0042, 'learning_rate': 2.1418e-09, 'epoch': 1.99, 'throughput': 2415.42}

[INFO|callbacks.py:310] 2024-10-04 23:10:54,528 >> {'loss': 0.0028, 'learning_rate': 0.0000e+00, 'epoch': 2.00, 'throughput': 2415.63}

[INFO|trainer.py:3478] 2024-10-04 23:10:54,529 >> Saving model checkpoint to saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1200

[INFO|tokenization_utils_base.py:2574] 2024-10-04 23:10:54,663 >> tokenizer config file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1200/tokenizer_config.json

[INFO|tokenization_utils_base.py:2583] 2024-10-04 23:10:54,663 >> Special tokens file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/checkpoint-1200/special_tokens_map.json

[INFO|trainer.py:2383] 2024-10-04 23:10:55,007 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|trainer.py:3478] 2024-10-04 23:10:55,010 >> Saving model checkpoint to saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36

[INFO|tokenization_utils_base.py:2574] 2024-10-04 23:10:55,132 >> tokenizer config file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/tokenizer_config.json

[INFO|tokenization_utils_base.py:2583] 2024-10-04 23:10:55,133 >> Special tokens file saved in saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-10-04-21-06-36/special_tokens_map.json

[WARNING|ploting.py:89] 2024-10-04 23:10:55,400 >> No metric eval_loss to plot.

[WARNING|ploting.py:89] 2024-10-04 23:10:55,400 >> No metric eval_accuracy to plot.

[INFO|modelcard.py:449] 2024-10-04 23:10:55,401 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

